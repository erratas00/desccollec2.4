"""
GÉNÉRATEUR DE DESCRIPTIONS COLLECTIONS E-COMMERCE SEO
Script complet et autonome - Prêt à l'emploi
"""

from __future__ import annotations

import argparse
import html
import html.parser
import json
import os
import random
import re
import statistics
import textwrap
import time
import unicodedata
import urllib.error
import urllib.parse
import urllib.request
from collections import Counter, defaultdict
from functools import lru_cache
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

from openai import OpenAI
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

try:  # pragma: no cover - dépend des modules installés
    import trafilatura
except Exception:  # noqa: BLE001 - tolérance si non disponible
    trafilatura = None  # type: ignore[assignment]

try:  # pragma: no cover - dépend des modules installés
    import spacy
except Exception:  # noqa: BLE001
    spacy = None  # type: ignore[assignment]

try:  # pragma: no cover - dépend des modules installés
    from sentence_transformers import SentenceTransformer
except Exception:  # noqa: BLE001
    SentenceTransformer = None  # type: ignore[assignment]

try:  # pragma: no cover - dépend de l'environnement d'exécution
    import numpy as np
except Exception:  # noqa: BLE001 - toute erreur doit être gérée
    np = None  # type: ignore[assignment]

try:  # pragma: no cover - dépend de l'environnement d'exécution
    from sklearn.cluster import KMeans
    from sklearn.feature_extraction.text import TfidfVectorizer
except Exception:  # noqa: BLE001
    KMeans = None  # type: ignore[assignment]
    TfidfVectorizer = None  # type: ignore[assignment]

# ✅ FIX SSL pour macOS - AJOUTE CES 3 LIGNES
import ssl
import certifi
ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())
# FIN DU FIX SSL

# ============================================================================
# CONFIGURATION GLOBALE
# ============================================================================

CONFIG: Dict[str, object] = {
    "target_words_min": 1200,
    "intent_labels": ["info", "transac", "comparatif", "guide"],
    "ban_phrases": [
        "notre collection",
        "sublimer",
        "expérience",
        "exceptionnelle",
        "parfait pour",
        "idéal pour",
        "sans compromis",
        "alliant style et",
        "une touche d'",
        "notre sélection",
    ],
    "tone": "pédagogique_expert_non_promo",
    "word_count_variance": 0.1,
}

# ============================================================================
# CONFIGURATION API GOOGLE CUSTOM SEARCH
# ============================================================================



API_KEYS = [
    "AIzaSyAuRYhKsXWWP7LnrpRk5xlyfqBh9SPU0VU",
    "AIzaSyAmQIoVerxVIvQZ4PHQPRZEvQ6ZOu8dork",
    "AIzaSyAp7B9mif-BTsxjQoUUVOhe-YmuCJVTZvU",
    "AIzaSyDJzk39tPbJ_V0V4Rh1kG1LeZ_h0ImPcV8",
    "AIzaSyAo6Cnfl05Hzp2aBDG3sghAi-XsYEOHQGw",
    "AIzaSyBG22uIPrlOPiVfnGa6bXHgfdiUpQxLqkc",
    "AIzaSyCN3CdD2BMYxen63fqvtM58LvPWGD6KnP0",
    "AIzaSyDbyO4prNC6-GQizuFt_lyuDy5n-Bdj2h4",
    "AIzaSyCgMYPJIqGBFadCk3NO6f18E1kZ5yAKElg",
    "AIzaSyCB_RIO0NwchJ4ULRCVmzIPDWZmrQ6ROXY",
    "AIzaSyB6n-S9N2UVM_JJxGJ14Bq52hG_aJzmec8",
    "AIzaSyA_uAtyB96buwzDqxDGRgYkoWFEW2cxhKU",
    "AIzaSyDnme-Gq3KT2j3PY33N4kKdVA64WM29q74",
    "AIzaSyDGvCdLNut9zhVqQMjG9jVAEu9GNK98cc",
    "AIzaSyBMo-Eh2-Recy9gmJlfOF4RJWMOokHEXbM",
    "AIzaSyCWzelz-WBEA9dikOUncpk_nHee5VC4bMw",
    "AIzaSyCvIQB5-YoJt8MVX3C1B6uTq3qqOjWox8I",
    "AIzaSyDoo50GTN4efYc77M_NJ5pvLySLO5mREh4",
    "AIzaSyCswokAwEK1hTG_pY1FFYceebCeGwbws9M",
    "AIzaSyBFMZywGryzSQvvXc_BM_2xie0Vc_ok4A0",
]

CX_ID = "0745b59b812b94795"
API_URL = "https://www.googleapis.com/customsearch/v1"
API_KEY_PAUSE_SECONDS = 3600


# ============================================================================
# GESTION DES CLÉS API
# ============================================================================


class _QuotaExceededError(RuntimeError):
    """Erreur levée lorsque la limite de quota de l'API est atteinte."""


class _ApiRequestError(RuntimeError):
    """Erreur générique pour les appels à l'API Google Custom Search."""


class _ApiKeyManager:
    """Gère l'alternance et la mise en pause des clés API."""

    def __init__(self, keys: Sequence[str], pause_duration: int = API_KEY_PAUSE_SECONDS) -> None:
        uniques: List[str] = []
        for key in keys:
            if key and key not in uniques:
                uniques.append(key)
        self._keys = uniques
        self._pause_duration = pause_duration
        self._paused_until = {key: 0.0 for key in uniques}
        self._index = 0

    def prochaine_cle(self) -> Optional[str]:
        """Retourne la prochaine clé disponible en respectant l'ordre d'alternance."""

        if not self._keys:
            return None

        maintenant = time.time()
        for _ in range(len(self._keys)):
            cle = self._keys[self._index]
            self._index = (self._index + 1) % len(self._keys)
            if maintenant >= self._paused_until.get(cle, 0.0):
                return cle
        return None

    def mettre_en_pause(self, cle: str, duree: Optional[int] = None) -> None:
        """Met une clé en pause pour la durée spécifiée (en secondes)."""

        if cle in self._paused_until:
            self._paused_until[cle] = time.time() + (duree or self._pause_duration)


_STOPWORDS_FR = {
    "alors",
    "au",
    "aucun",
    "aussi",
    "autre",
    "avant",
    "avec",
    "avoir",
    "bon",
    "car",
    "ce",
    "cela",
    "ces",
    "cet",
    "cette",
    "ceux",
    "chaque",
    "comme",
    "comment",
    "dans",
    "de",
    "des",
    "du",
    "donc",
    "dont",
    "elle",
    "elles",
    "en",
    "encore",
    "entre",
    "est",
    "et",
    "eux",
    "faire",
    "faut",
    "hors",
    "ici",
    "ils",
    "je",
    "la",
    "le",
    "les",
    "leur",
    "lors",
    "lui",
    "mais",
    "mal",
    "mes",
    "moi",
    "mon",
    "même",
    "nos",
    "notre",
    "nous",
    "ou",
    "où",
    "par",
    "parce",
    "pas",
    "peu",
    "plus",
    "pour",
    "quand",
    "que",
    "quel",
    "quelle",
    "quelles",
    "quels",
    "qui",
    "sa",
    "sans",
    "se",
    "ses",
    "si",
    "son",
    "sont",
    "sur",
    "ta",
    "tandis",
    "te",
    "tes",
    "toi",
    "ton",
    "toujours",
    "tous",
    "tout",
    "très",
    "tu",
    "un",
    "une",
    "vos",
    "votre",
    "vous",
    "y",
}


_CARACTERISTIQUES_PATTERNS = {
    "couleur": [
        "beige",
        "blanc",
        "bleu",
        "bleu canard",
        "bleu marine",
        "bordeaux",
        "corail",
        "cuivre",
        "doré",
        "gris",
        "ivoire",
        "jaune",
        "kaki",
        "lavande",
        "moutarde",
        "noir",
        "pastel",
        "rose",
        "rouge",
        "sable",
        "taupe",
        "terracotta",
        "turquoise",
        "vert",
        "vert sauge",
    ],
    "style": [
        "art déco",
        "bohème",
        "campagne chic",
        "contemporain",
        "design",
        "industriel",
        "minimaliste",
        "naturel",
        "scandinave",
        "vintage",
    ],
    "public": [
        "adulte",
        "bébé",
        "enfant",
        "femme",
        "fille",
        "garçon",
        "homme",
        "mixte",
    ],
    "matière": [
        "bambou",
        "bois",
        "coton",
        "jute",
        "laine",
        "lin",
        "microfibre",
        "velours",
    ],
    "motif": [
        "géométrique",
        "uni",
        "à rayures",
        "à pois",
        "floral",
        "imprimé",
    ],
}


def _identifier_caracteristiques(titre: str) -> List[Tuple[str, str]]:
    """Repère les attributs distinctifs (couleur, style, cible…) dans un titre."""

    if not titre:
        return []

    titre_min = titre.lower()
    titre_normalise = re.sub(r"[-/]+", " ", titre_min)
    trouves: List[Tuple[str, str]] = []
    deja_vus: set[str] = set()

    def _ajouter(categorie: str, valeur: str) -> None:
        cle = f"{categorie}:{valeur}"
        if cle not in deja_vus:
            trouves.append((categorie, valeur))
            deja_vus.add(cle)

    for categorie, motifs in _CARACTERISTIQUES_PATTERNS.items():
        for motif in motifs:
            motif_min = motif.lower()
            motif_regex = re.compile(rf"(?<!\w){re.escape(motif_min)}(?!\w)")
            if motif_regex.search(titre_normalise):
                _ajouter(categorie, motif)

    if not trouves:
        candidats = [
            mot
            for mot in re.findall(r"[\wÀ-ÖØ-öø-ÿ']+", titre_normalise)
            if mot not in _STOPWORDS_FR
        ]
        if len(candidats) >= 2:
            _ajouter("qualificatif", candidats[-1])

    return trouves


def _liste_naturelle(elements: Sequence[str]) -> str:
    """Formate une liste courte avec des virgules et « et » final."""

    uniques: List[str] = []
    vus: set[str] = set()
    for element in elements:
        cle = element.lower()
        if cle not in vus:
            uniques.append(element)
            vus.add(cle)

    if not uniques:
        return ""
    if len(uniques) == 1:
        return uniques[0]
    if len(uniques) == 2:
        return f"{uniques[0]} et {uniques[1]}"
    return ", ".join(uniques[:-1]) + f" et {uniques[-1]}"


def _fusionner_caracteristiques(
    *listes: Optional[Sequence[Tuple[str, str]]]
) -> List[Tuple[str, str]]:
    """Fusionne des listes de caractéristiques en supprimant les doublons."""

    resultat: List[Tuple[str, str]] = []
    deja_vus: set[Tuple[str, str]] = set()
    for liste in listes:
        if not liste:
            continue
        for categorie, valeur in liste:
            categorie_propre = (categorie or "").strip()
            valeur_propre = (valeur or "").strip()
            if not categorie_propre or not valeur_propre:
                continue
            cle = (categorie_propre.lower(), valeur_propre.lower())
            if cle not in deja_vus:
                resultat.append((categorie_propre, valeur_propre))
                deja_vus.add(cle)
    return resultat


def _bloc_caracteristiques_prompt(
    caracteristiques: Sequence[Tuple[str, str]],
    focus_unique: Optional[str] = None,
) -> str:
    """Construit un bloc d'instructions centré sur la différenciation."""

    if not caracteristiques and not focus_unique:
        return ""

    details: List[str] = []
    valeurs: List[str] = []
    for categorie, valeur in caracteristiques:
        valeurs.append(valeur)
        if categorie == "couleur":
            details.append(f"couleur « {valeur} »")
        elif categorie == "style":
            details.append(f"style {valeur}")
        elif categorie == "public":
            details.append(f"cible {valeur}")
        elif categorie == "matière":
            details.append(f"matière {valeur}")
        elif categorie == "motif":
            details.append(f"motif {valeur}")
        else:
            details.append(valeur)

    focus_central = _liste_naturelle(details) if details else (focus_unique or "")
    angle = _liste_naturelle(valeurs) if valeurs else (focus_unique or "")

    return textwrap.dedent(
        f"""
        ANGLE DIFFÉRENCIANT À RENFORCER :
        - Atout central : {focus_central}.
        - Faire apparaître cette spécificité ({angle}) dans chaque titre de pilier avec un adjectif sensoriel ou descriptif distinct.
        - Dès l'introduction, expliquer concrètement ce que cette caractéristique change pour le client au quotidien (confort, ambiance, entretien...).
        - Relier chaque argument à cette spécificité ({angle}) : ambiance déco, combinaisons matière/couleur, situations d'usage.
        - Comparer avec les autres variantes seulement pour souligner pourquoi cette option est unique.
        - Employer un vocabulaire sensoriel et des bénéfices clients pour ancrer cette différence dans l'esprit du lecteur.
        - Conclure par une synthèse rappelant clairement la promesse unique liée à {focus_unique or angle}.
        """
    ).strip()


# ============================================================================
# STRUCTURES DE DONNÉES
# ============================================================================


@dataclass(slots=True)
class CollectionRequest:
    """Représente les informations nécessaires pour générer une collection."""

    titre_collection: str
    mots_cles_principaux: List[str] = field(default_factory=list)
    mots_cles_secondaires: List[str] = field(default_factory=list)
    questions_faq: List[str] = field(default_factory=list)
    infos_supplementaires: Optional[str] = None
    liens_internes: List[str] = field(default_factory=list)
    requete_serp: Optional[str] = None
    caracteristiques: List[Tuple[str, str]] = field(default_factory=list)
    focus_unique: Optional[str] = None
    guidelines_differenciation: Optional[str] = None

    def __post_init__(self) -> None:
        self.titre_collection = self.titre_collection.strip()
        if not self.titre_collection:
            raise ValueError("Le titre de la collection est obligatoire")

        self.mots_cles_principaux = _normaliser_liste(
            self.mots_cles_principaux or [self.titre_collection.lower()]
        )
        self.mots_cles_secondaires = _normaliser_liste(self.mots_cles_secondaires)
        self.questions_faq = _normaliser_liste(self.questions_faq)
        self.liens_internes = _normaliser_liste(self.liens_internes)
        requete_serp = (self.requete_serp or self.titre_collection).strip()
        self.requete_serp = requete_serp or self.titre_collection
        caracteristiques_detectees = _identifier_caracteristiques(self.titre_collection)
        self.caracteristiques = _fusionner_caracteristiques(
            self.caracteristiques,
            caracteristiques_detectees,
        )

        if self.focus_unique:
            self.focus_unique = self.focus_unique.strip()
        if not self.focus_unique and self.caracteristiques:
            self.focus_unique = self.caracteristiques[0][1]

        if self.guidelines_differenciation:
            self.guidelines_differenciation = textwrap.dedent(
                self.guidelines_differenciation
            ).strip()

        if len(self.questions_faq) < 4:
            base_questions = [
                f"Comment choisir {self.titre_collection.lower()} ?",
                f"Quels sont les avantages de {self.titre_collection.lower()} ?",
                f"Comment entretenir {self.titre_collection.lower()} ?",
                f"Quels critères comparer avant d'acheter {self.titre_collection.lower()} ?",
            ]
            for question in base_questions:
                if question not in self.questions_faq:
                    self.questions_faq.append(question)
                if len(self.questions_faq) >= 4:
                    break


# ============================================================================
# PRÉPARATION MULTI-COLLECTIONS
# ============================================================================


def _extraire_tokens_significatifs(titre: str) -> List[str]:
    """Retourne les mots clés utiles d'un titre en excluant les stopwords."""

    if not titre:
        return []

    tokens = re.findall(r"[\wÀ-ÖØ-öø-ÿ']+", titre.lower())
    return [token for token in tokens if token and token not in _STOPWORDS_FR]


def _prefixe_tokens_commun(listes: Sequence[Sequence[str]]) -> List[str]:
    """Identifie le préfixe commun à plusieurs séquences de tokens."""

    if not listes:
        return []

    prefixe: List[str] = list(listes[0])
    for tokens in listes[1:]:
        max_len = min(len(prefixe), len(tokens))
        idx = 0
        while idx < max_len and prefixe[idx] == tokens[idx]:
            idx += 1
        prefixe = prefixe[:idx]
        if not prefixe:
            break
    return prefixe


def _segment_unique_titre(titre: str, prefix_tokens: Sequence[str]) -> str:
    """Extraie la portion distinctive d'un titre après le préfixe commun."""

    if not titre:
        return ""
    if not prefix_tokens:
        return ""

    prefixe_phrase = " ".join(prefix_tokens).strip()
    if not prefixe_phrase:
        return ""

    motif = re.compile(rf"^\s*{re.escape(prefixe_phrase)}[\s\-:|]*", re.IGNORECASE)
    segment = motif.sub("", titre).strip(" -:|")
    return segment


def _preparer_differenciation_collections(
    requetes: Sequence["CollectionRequest"],
) -> None:
    """Complète les collections avec un focus unique pour éviter la cannibalisation."""

    requetes_valides = [req for req in requetes if getattr(req, "titre_collection", "")]
    if not requetes_valides:
        return

    tokens_list = [_extraire_tokens_significatifs(req.titre_collection) for req in requetes_valides]
    prefix_tokens = _prefixe_tokens_commun(tokens_list) if len(requetes_valides) > 1 else []

    compteur: Counter[Tuple[str, str]] = Counter()
    for req in requetes_valides:
        for categorie, valeur in req.caracteristiques:
            if not categorie or not valeur:
                continue
            compteur[(categorie.lower(), valeur.lower())] += 1

    focus_mapping: Dict[str, str] = {}

    for req in requetes_valides:
        uniques_existants = [
            (categorie, valeur)
            for categorie, valeur in req.caracteristiques
            if categorie
            and valeur
            and compteur.get((categorie.lower(), valeur.lower()), 0) == 1
        ]

        segment_unique = _segment_unique_titre(req.titre_collection, prefix_tokens)
        uniques = list(uniques_existants)

        if segment_unique:
            uniques = _fusionner_caracteristiques(
                uniques,
                [("différenciateur", segment_unique)],
            )

        if not uniques and req.caracteristiques:
            uniques = [req.caracteristiques[0]]

        focus = _liste_naturelle([valeur for _, valeur in uniques]) if uniques else ""
        if not focus:
            focus = segment_unique or req.focus_unique or req.titre_collection

        if focus:
            req.focus_unique = focus
            focus_mapping[req.titre_collection] = focus
        else:
            focus_mapping[req.titre_collection] = req.titre_collection

        req.caracteristiques = _fusionner_caracteristiques(req.caracteristiques, uniques)

    if len(focus_mapping) <= 1:
        return

    for req in requetes_valides:
        autres = [
            f"• {titre} → {focus_mapping[titre]}"
            for titre in focus_mapping
            if titre != req.titre_collection
        ]
        if not autres:
            continue

        bloc = (
            "ÉVITER LA CANNIBALISATION :\n"
            "- Ne pas dupliquer les angles dominants des collections suivantes :\n"
            + "\n".join(autres)
            + f"\n- Ancrer systématiquement la différence autour de : {focus_mapping[req.titre_collection]}.\n"
            "- Les comparaisons doivent rappeler pourquoi cette version est unique."
        )

        if req.guidelines_differenciation:
            bloc = req.guidelines_differenciation + "\n" + bloc

        req.guidelines_differenciation = textwrap.dedent(bloc).strip()


# ============================================================================
# OUTILS
# ============================================================================


def _normaliser_liste(valeurs: Optional[Iterable[str]]) -> List[str]:
    """Nettoie les chaînes et supprime les doublons tout en conservant l'ordre."""

    if not valeurs:
        return []

    resultat: List[str] = []
    deja_vus = set()
    for valeur in valeurs:
        propre = (valeur or "").strip()
        if propre and propre.lower() not in deja_vus:
            resultat.append(propre)
            deja_vus.add(propre.lower())
    return resultat


def _detecter_intention(texte: str) -> str:
    """Identifie une intention éditoriale basique à partir du texte cible."""

    texte_min = texte.lower()
    if any(mot in texte_min for mot in ("acheter", "prix", "livraison", "panier")):
        return "transac"
    if any(mot in texte_min for mot in ("comparatif", "versus", "meilleur", "top")):
        return "comparatif"
    if any(mot in texte_min for mot in ("guide", "comment", "conseil", "astuce")):
        return "guide"
    return "info"


def _condense_phrase(texte: str, limite: int = 24) -> str:
    """Condense une phrase pour l'utiliser dans un meta titre/description."""

    propre = (texte or "").strip()
    if not propre:
        return ""
    propre = re.sub(r"[\s]+", " ", propre)
    propre = propre.strip("-•–|:")
    if not propre:
        return ""
    resultat = textwrap.shorten(propre, width=limite, placeholder="")
    return resultat.strip()


def _benefice_depuis_insight(insight: str) -> str:
    """Transforme un insight quantitatif en bénéfice court pour les metas."""

    if not insight:
        return ""
    insight_min = insight.lower()
    if "prix" in insight_min:
        return "Prix maîtrisés"
    if "matériaux" in insight_min:
        return "Matériaux experts"
    if "dimension" in insight_min:
        return "Dimensions maîtrisées"
    if "variation" in insight_min or "dispersion" in insight_min:
        return "Stabilité mesurée"
    segment = insight.split(":", 1)[-1] if ":" in insight else insight
    segment = segment.split("(", 1)[0].strip()
    return _condense_phrase(segment, limite=26)


def _formater_avantage(insight: str) -> str:
    """Formate un insight en avantage court pour la meta description."""

    if not insight:
        return ""
    titre, separateur, detail = insight.partition(":")
    titre_min = titre.lower()
    detail = detail.strip() or titre.strip()
    if "prix" in titre_min:
        principal = detail.split("(", 1)[0].strip() or detail
        texte = f"Prix maîtrisés {principal}".strip()
    elif "matériaux" in titre_min:
        texte = f"Matériaux dominants : {detail}".strip()
    elif "dimension" in titre_min:
        texte = f"Dimensions typiques {detail}".strip()
    elif "variation" in titre_min or "dispersion" in titre_min:
        texte = f"Variation contrôlée {detail}".strip()
    else:
        base = detail or insight
        texte = _condense_phrase(base, limite=44)
    texte = texte.strip()
    if not texte:
        return ""
    return texte[0].upper() + texte[1:]


def _generer_meta_title(
    titre_collection: str,
    analyse_serp: Optional["AnalyseSerp"] = None,
    focus_unique: Optional[str] = None,
) -> str:
    """Construit un meta title basé sur la SERP tout en restant ≤ 60 caractères."""

    intent = analyse_serp.intent_label if analyse_serp else _detecter_intention(titre_collection)
    annee = datetime.now().year

    benefit_candidates: List[str] = []
    if focus_unique:
        benefit_candidates.append(_condense_phrase(f"{focus_unique.capitalize()} durable", limite=28))
    if analyse_serp:
        for insight in analyse_serp.insights_structurels[:3]:
            benefice = _benefice_depuis_insight(insight)
            if benefice and benefice not in benefit_candidates:
                benefit_candidates.append(benefice)
        if analyse_serp.domaines:
            premier = analyse_serp.domaines[0]
            candidats = premier.sous_themes[:2] or premier.termes_dominants[:2]
            for terme in candidats:
                resume = _condense_phrase(terme, limite=26)
                if resume and resume not in benefit_candidates:
                    benefit_candidates.append(resume.capitalize())
        if analyse_serp.suggestions:
            suggestion = _condense_phrase(analyse_serp.suggestions[0], limite=24)
            if suggestion and suggestion not in benefit_candidates:
                benefit_candidates.append(suggestion.capitalize())

    benefit = next((cand for cand in benefit_candidates if cand), None)
    if not benefit:
        benefit = "Confort durable"

    suffix_map = {
        "transac": f"Tests & avis {annee}",
        "comparatif": f"Comparatif expert {annee}",
        "guide": f"Guide expert {annee}",
        "info": f"Tendances {annee}",
    }
    suffix = suffix_map.get(intent, f"Sélection {annee}")
    if analyse_serp and analyse_serp.insights_structurels:
        suffix = f"Données {annee}"

    meta = f"{titre_collection} – {benefit} | {suffix}".strip()
    if len(meta) > 60:
        benefit = _condense_phrase(benefit, limite=18)
        meta = f"{titre_collection} – {benefit} | {suffix}".strip()
    if len(meta) > 60:
        meta = f"{titre_collection} | {suffix}".strip()
    if len(meta) > 60:
        meta = textwrap.shorten(meta, width=60, placeholder="…")

    return meta


def _generer_meta_description(
    titre_collection: str,
    mots_cles: Sequence[str],
    caracteristiques: Optional[Sequence[Tuple[str, str]]] = None,
    analyse_serp: Optional["AnalyseSerp"] = None,
    focus_unique: Optional[str] = None,
) -> str:
    """Crée une meta description engageante (~150 caractères)."""

    intent = analyse_serp.intent_label if analyse_serp else _detecter_intention(titre_collection)
    mot_cle = mots_cles[0] if mots_cles else titre_collection.lower()
    focus_valeurs = [valeur for _, valeur in caracteristiques] if caracteristiques else []
    focus_phrase = focus_unique or _liste_naturelle(focus_valeurs)

    base_focus = (focus_phrase or mot_cle).lower()
    probleme = f"les {base_focus} qui déçoivent"
    if focus_unique:
        probleme = f"les {titre_collection.lower()} sans {focus_unique.lower()}"

    avantages: List[str] = []
    if analyse_serp:
        for insight in analyse_serp.insights_structurels:
            avantage = _formater_avantage(insight)
            if avantage and avantage not in avantages:
                avantages.append(avantage)
            if len(avantages) >= 2:
                break
    if focus_phrase and len(avantages) < 2:
        avantage_focus = f"{focus_phrase} maîtrisé".strip()
        avantage_focus = avantage_focus[0].upper() + avantage_focus[1:] if avantage_focus else ""
        if avantage_focus and avantage_focus not in avantages:
            avantages.append(avantage_focus)
    domaines = analyse_serp.domaines if analyse_serp else []
    if domaines and len(avantages) < 2:
        titre = _condense_phrase(domaines[0].titre, limite=40)
        if titre and titre not in avantages:
            avantages.append(titre)

    fallback_map = {
        "transac": ["Livraison suivie", "Stock immédiat"],
        "comparatif": ["Comparaison rapide", "Repères clairs"],
        "guide": ["Conseils d'entretien", "Usage maîtrisé"],
        "info": ["Inspiration assurée", "Tendances décryptées"],
        "default": ["Sélection experte", "Finition soignée"],
    }
    while len(avantages) < 2:
        options = fallback_map.get(intent, fallback_map["default"])
        ajoute = False
        for option in options:
            if option not in avantages:
                avantages.append(option)
                ajoute = True
                break
        if not ajoute:
            for option in fallback_map["default"]:
                if option not in avantages:
                    avantages.append(option)
                    ajoute = True
                    break
        if not ajoute:
            break

    qualite = "testés par nos experts"
    produit = titre_collection.lower()

    cta_map = {
        "transac": "Profitez des nouveautés",
        "comparatif": "Comparez les modèles",
        "guide": "Explorez nos conseils",
        "info": "Découvrez la sélection",
    }
    cta = cta_map.get(intent, "Découvrez la sélection")

    description = (
        f"Fini {probleme}. "
        f"Découvrez nos {produit} {qualite} : {avantages[0]}, {avantages[1]}. "
        f"{cta}."
    )

    description = re.sub(r"\s+", " ", description).strip()
    if len(description) > 160:
        description = textwrap.shorten(description, width=157, placeholder="…")

    return description


@dataclass(slots=True)
class SerpExtract:
    """Représente un extrait pertinent issu de la SERP."""

    titre: str
    url: str
    snippet: str
    extrait_contenu: str
    texte_integral: str
    structured_data: List[Dict[str, str]] = field(default_factory=list)


@dataclass(slots=True)
class DomaineSemantique:
    """Représente un pilier éditorial identifié dans la SERP."""

    titre: str
    termes_dominants: List[str]
    sous_themes: List[str]
    resume: str
    poids: float
    urls: List[str]
    entites: List[str] = field(default_factory=list)
    faits: List[str] = field(default_factory=list)
    questions: List[str] = field(default_factory=list)
    checklist: List[str] = field(default_factory=list)
    table_rows: List[Dict[str, str]] = field(default_factory=list)
    word_budget: int = 0


@dataclass(slots=True)
class AnalyseSerp:
    """Synthèse structurée du corpus SERP."""

    resume: str
    domaines: List[DomaineSemantique]
    sources: List[dict]
    texte_prompt: str
    plan_sections: List[dict] = field(default_factory=list)
    questions_paa: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    intent_label: str = "info"
    donnees_structurees: List[Dict[str, str]] = field(default_factory=list)
    insights_structurels: List[str] = field(default_factory=list)

    def as_dict(self) -> dict:
        return {
            "resume": self.resume,
            "domaines": [
                {
                    "titre": domaine.titre,
                    "termes_dominants": domaine.termes_dominants,
                    "sous_themes": domaine.sous_themes,
                    "resume": domaine.resume,
                    "poids": domaine.poids,
                    "urls": domaine.urls,
                    "entites": domaine.entites,
                    "faits": domaine.faits,
                    "questions": domaine.questions,
                    "checklist": domaine.checklist,
                    "table_rows": domaine.table_rows,
                    "word_budget": domaine.word_budget,
                }
                for domaine in self.domaines
            ],
            "sources": self.sources,
            "texte_prompt": self.texte_prompt,
            "plan_sections": self.plan_sections,
            "questions_paa": self.questions_paa,
            "suggestions": self.suggestions,
            "intent_label": self.intent_label,
            "donnees_structurees": self.donnees_structurees,
            "insights_structurels": self.insights_structurels,
        }


@dataclass(slots=True)
class SectionSemantique:
    """Représente une section structurée extraite d'un document SERP."""

    titre: str
    texte: str
    source_index: int
    url: str
    entites: List[str] = field(default_factory=list)
    faits: List[str] = field(default_factory=list)
    questions: List[str] = field(default_factory=list)


def _tronquer_texte(texte: str, max_mots: int = 80) -> str:
    """Limite un texte à un nombre de mots raisonnable pour le prompt."""

    mots = texte.split()
    if len(mots) <= max_mots:
        return " ".join(mots)
    tronque = " ".join(mots[:max_mots])
    return tronque + " …"


def _extraire_texte_html(contenu_html: str) -> str:
    """Transforme un contenu HTML en texte brut nettoyé."""

    class _TextExtractor(html.parser.HTMLParser):
        def __init__(self) -> None:
            super().__init__()
            self._segments: List[str] = []
            self._skip_level = 0

        def handle_starttag(self, tag: str, attrs) -> None:  # noqa: ANN001
            if tag in {"script", "style", "noscript"}:
                self._skip_level += 1

        def handle_endtag(self, tag: str) -> None:
            if tag in {"script", "style", "noscript"} and self._skip_level > 0:
                self._skip_level -= 1

        def handle_data(self, data: str) -> None:
            if self._skip_level == 0:
                texte = data.strip()
                if texte:
                    self._segments.append(texte)

        def get_text(self) -> str:
            texte = " ".join(self._segments)
            texte = re.sub(r"\s+", " ", texte)
            return texte.strip()

    parser = _TextExtractor()
    parser.feed(contenu_html)
    return parser.get_text()


@lru_cache(maxsize=1)
def _charger_modele_spacy():
    """Charge un modèle spaCy français si disponible."""

    if spacy is None:  # pragma: no cover - dépendances optionnelles
        return None

    for modele in ("fr_core_news_md", "fr_core_news_lg", "fr_core_news_sm"):
        try:
            return spacy.load(modele)  # type: ignore[return-value]
        except Exception:  # noqa: BLE001
            continue
    return None


def _extraire_entites(texte: str) -> List[str]:
    """Extraction d'entités nommées et groupes nominaux."""

    nlp = _charger_modele_spacy()
    if not nlp or not texte:
        return []

    doc = nlp(texte)
    entites = {ent.text.strip() for ent in doc.ents if ent.label_ not in {"PUNCT", "SPACE"}}
    entites.update(chunk.text.strip() for chunk in doc.noun_chunks if len(chunk.text.strip()) > 2)
    resultat = [ent for ent in entites if len(ent.split()) <= 6]
    return sorted(resultat)


_FAIT_REGEX = re.compile(r"[^.?!]*\d[^.?!]*[.?!]")
_QUESTION_REGEX = re.compile(r"[^.?!?]*\?+\s*")


def _extraire_faits(texte: str) -> List[str]:
    """Identifie les phrases contenant chiffres, normes ou dimensions."""

    if not texte:
        return []
    faits = [phrase.strip() for phrase in _FAIT_REGEX.findall(texte) if len(phrase.strip().split()) >= 4]
    return faits[:12]


def _extraire_questions(texte: str) -> List[str]:
    """Identifie les questions potentielles présentes dans le texte."""

    if not texte:
        return []
    questions = [q.strip() for q in _QUESTION_REGEX.findall(texte)]
    return [q for q in questions if 8 <= len(q.split()) <= 20][:8]


def _scinder_sections(extrait: SerpExtract) -> List[SectionSemantique]:
    """Découpe un extrait SERP en sections exploitables."""

    contenu = extrait.texte_integral or extrait.extrait_contenu or extrait.snippet
    if not contenu:
        return []

    segments = re.split(r"\n{2,}|(?<=\.)\s{2,}", contenu)
    sections: List[SectionSemantique] = []
    for index, segment in enumerate(segments):
        propre = segment.strip()
        if len(propre) < 120:
            continue
        titre = propre.split(".")[0][:80]
        entites = _extraire_entites(propre)
        faits = _extraire_faits(propre)
        questions = _extraire_questions(propre)
        sections.append(
            SectionSemantique(
                titre=titre or extrait.titre,
                texte=propre,
                source_index=index,
                url=extrait.url,
                entites=entites,
                faits=faits,
                questions=questions,
            )
        )
    return sections


@lru_cache(maxsize=1)
def _charger_modele_embeddings():
    """Charge le modèle SentenceTransformer si disponible."""

    if SentenceTransformer is None:  # pragma: no cover
        return None
    try:
        return SentenceTransformer("all-MiniLM-L6-v2")
    except Exception:  # noqa: BLE001
        return None


def _embed_sections(textes: Sequence[str]) -> Optional["np.ndarray"]:
    """Génère des embeddings normalisés pour une liste de textes."""

    if np is None or not textes:
        return None

    modele = _charger_modele_embeddings()
    if modele is None:
        return None

    try:
        vecteurs = modele.encode(list(textes), normalize_embeddings=True)
        return np.asarray(vecteurs)
    except Exception:  # noqa: BLE001
        return None


def _selectionner_piliers_mmr(
    embeddings: "np.ndarray", k: int = 10, lambda_: float = 0.7
) -> List[int]:
    """Sélection de centres thématiques via MMR."""

    if np is None or embeddings.size == 0:
        return []

    k = min(k, embeddings.shape[0])
    if k <= 0:
        return []

    centroid = embeddings.mean(axis=0)
    sims = embeddings @ centroid
    selection: List[int] = []
    remaining = list(range(embeddings.shape[0]))
    first = int(np.argmax(sims))
    selection.append(first)
    remaining.remove(first)

    while remaining and len(selection) < k:
        best_score = -1.0
        best_candidate = remaining[0]
        for candidat in remaining:
            pertinence = sims[candidat]
            diversite = max(
                float(embeddings[candidat] @ embeddings[sel]) for sel in selection
            )
            score = lambda_ * pertinence - (1 - lambda_) * diversite
            if score > best_score:
                best_score = score
                best_candidate = candidat
        selection.append(best_candidate)
        remaining.remove(best_candidate)

    return selection


_PAIR_REGEX = re.compile(r"(?P<cle>[\wÀ-ÖØ-öø-ÿ\s]{3,40})\s*[:\-]\s*(?P<valeur>[^.;\n]{2,60})")

_PRICE_VALUE_REGEX = re.compile(r"(\d+[\d\s]*(?:[.,]\d+)?)")
_MATIERE_SEPARATORS = re.compile(r"[\s]*[,;/][\s]*|\bet\b", re.IGNORECASE)


def _extraire_pairs(texte: str) -> List[Dict[str, str]]:
    """Extrait des couples clé/valeur pour alimenter des tableaux comparatifs."""

    resultats: List[Dict[str, str]] = []
    for match in _PAIR_REGEX.finditer(texte):
        cle = match.group("cle").strip()
        valeur = match.group("valeur").strip()
        if cle and valeur:
            resultats.append({"clé": cle, "valeur": valeur})
    return resultats


def _schema_value(bloc: Dict[str, object], *cles: str) -> Optional[object]:
    """Récupère une valeur case-insensitive dans un bloc de données schema.org."""

    if not isinstance(bloc, dict):
        return None
    for cle in cles:
        cle_min = cle.lower()
        for key, valeur in bloc.items():
            if isinstance(key, str) and key.lower() == cle_min:
                return valeur
    return None


def _normaliser_schema_valeur(valeur: object) -> str:
    """Convertit une valeur schema.org hétérogène en chaîne propre."""

    if valeur is None:
        return ""
    if isinstance(valeur, (list, tuple, set)):
        morceaux = [_normaliser_schema_valeur(v) for v in valeur if v]
        return ", ".join([m for m in morceaux if m])
    if isinstance(valeur, dict):
        if "name" in valeur:
            return _normaliser_schema_valeur(valeur["name"])
        if "@value" in valeur:
            return _normaliser_schema_valeur(valeur["@value"])
        morceaux = [_normaliser_schema_valeur(v) for v in valeur.values() if v]
        return ", ".join([m for m in morceaux if m])
    texte = str(valeur).strip()
    return html.unescape(texte)


def _extraire_prix_schema(bloc: Dict[str, object]) -> str:
    """Formate un prix à partir d'un bloc schema.org."""

    brut = _schema_value(bloc, "price", "lowprice", "highprice", "pricevalue")
    if isinstance(brut, (list, tuple)) and brut:
        brut = brut[0]
    prix = _normaliser_schema_valeur(brut)
    devise = _normaliser_schema_valeur(
        _schema_value(bloc, "pricecurrency", "currency")
    )
    if prix and devise and devise.lower() not in prix.lower():
        return f"{prix} {devise}".strip()
    return prix.strip()


def _extraire_dimensions_schema(bloc: Dict[str, object]) -> str:
    """Assemble les dimensions pertinentes d'un bloc schema.org."""

    elements: List[str] = []
    for cle in ("size", "width", "height", "depth", "length", "weight"):
        valeur = _normaliser_schema_valeur(_schema_value(bloc, cle))
        if valeur:
            etiquette = "Dimensions" if cle == "size" else cle.capitalize()
            elements.append(f"{etiquette} {valeur}".strip())
    uniques: List[str] = []
    for element in elements:
        if element not in uniques:
            uniques.append(element)
    return ", ".join(uniques)


def _extraire_matiere_schema(bloc: Dict[str, object]) -> str:
    """Déduit les matériaux depuis un bloc schema.org."""

    matiere = _normaliser_schema_valeur(
        _schema_value(bloc, "material", "materials", "fabric", "color")
    )
    return matiere


def _extraire_nom_schema(bloc: Dict[str, object], fallback: str = "") -> str:
    """Tente de récupérer le nom d'un produit/élément."""

    nom = _normaliser_schema_valeur(
        _schema_value(bloc, "name", "title", "productname", "itemoffered")
    )
    return nom or fallback


def _construire_ligne_schema(
    bloc: Dict[str, object], label: str = "", fallback: Optional[Dict[str, str]] = None
) -> Optional[Dict[str, str]]:
    """Crée une ligne normalisée (Produit/Prix/Dimensions/Matière/Caractéristiques)."""

    if not isinstance(bloc, dict):
        return None

    ligne: Dict[str, str] = {
        "Produit": _extraire_nom_schema(bloc, fallback.get("Produit", "") if fallback else label),
        "Prix": _extraire_prix_schema(bloc),
        "Dimensions": _extraire_dimensions_schema(bloc),
        "Matière": _extraire_matiere_schema(bloc) or (fallback.get("Matière", "") if fallback else ""),
        "Caractéristiques": "",
    }

    marque = _normaliser_schema_valeur(_schema_value(bloc, "brand"))
    disponibilite = _normaliser_schema_valeur(
        _schema_value(bloc, "availability", "availabilitytext")
    )
    notation = _normaliser_schema_valeur(
        _schema_value(bloc, "ratingvalue", "aggregateRating", "reviewcount")
    )
    attributs = [texte for texte in (marque, disponibilite, notation) if texte]
    if fallback and not ligne["Caractéristiques"]:
        attributs.insert(0, fallback.get("Caractéristiques", ""))
    ligne["Caractéristiques"] = ", ".join([att for att in attributs if att]).strip(", ")

    if sum(bool(ligne[col]) for col in ("Produit", "Prix", "Dimensions", "Matière")) < 2:
        return None
    return ligne


def _extraire_donnees_structurees(pagemap: Dict[str, object]) -> List[Dict[str, str]]:
    """Convertit les données schema.org pertinentes en lignes de tableau."""

    lignes: List[Dict[str, str]] = []
    if not isinstance(pagemap, dict):
        return lignes

    def ajouter_ligne(bloc: Dict[str, object], fallback: Optional[Dict[str, str]] = None) -> None:
        ligne = _construire_ligne_schema(bloc, fallback=fallback or {})
        if ligne:
            lignes.append(ligne)

    produits = pagemap.get("product") or []
    if not isinstance(produits, list):
        produits = [produits]
    for produit in produits[:6]:
        ligne_produit = _construire_ligne_schema(produit, label="Produit") if isinstance(produit, dict) else None
        if ligne_produit:
            lignes.append(ligne_produit)
        offres = []
        if isinstance(produit, dict):
            brut_offres = _schema_value(produit, "offers")
            if brut_offres:
                if isinstance(brut_offres, list):
                    offres = [offre for offre in brut_offres if isinstance(offre, dict)]
                elif isinstance(brut_offres, dict):
                    offres = [brut_offres]
        for offre in offres[:4]:
            ajouter_ligne(offre, fallback=ligne_produit)

    offres_directes = pagemap.get("offer") or []
    if not isinstance(offres_directes, list):
        offres_directes = [offres_directes]
    for offre in offres_directes[:4]:
        ajouter_ligne(offre)

    agr_offres = pagemap.get("aggregateoffer") or []
    if not isinstance(agr_offres, list):
        agr_offres = [agr_offres]
    for bloc in agr_offres[:2]:
        ajouter_ligne(bloc)

    return lignes[:10]


def _generer_lignes_table(section: SectionSemantique) -> List[Dict[str, str]]:
    """Transforme les faits d'une section en lignes normalisées pour un tableau."""

    lignes: List[Dict[str, str]] = []
    type_label = section.titre[:60] or "Option"
    for fait in section.faits:
        texte_fait = fait.strip()
        if not texte_fait:
            continue
        ligne = {
            "Produit": type_label,
            "Dimensions": "",
            "Prix": "",
            "Matière": "",
            "Caractéristiques": texte_fait,
        }
        fait_min = texte_fait.lower()
        if re.search(r"\d+\s?(cm|mm|m|g/m²)", fait_min):
            ligne["Dimensions"] = texte_fait
        if re.search(r"\d+\s?€", texte_fait):
            ligne["Prix"] = texte_fait
        lignes.append(ligne)
        if len(lignes) >= 6:
            break
    return lignes


def make_table(rows: Sequence[Dict[str, str]]) -> str:
    """Construit une table HTML lorsque suffisamment de données sont disponibles."""

    colonnes = ["Produit", "Prix", "Dimensions", "Matière", "Caractéristiques"]
    colonnes_utiles = [col for col in colonnes if any(row.get(col) for row in rows)]
    if len(colonnes_utiles) < 3:
        return ""

    lignes_valides = []
    for row in rows:
        ligne = {col: row.get(col, "") for col in colonnes_utiles}
        if sum(bool(ligne[col]) for col in colonnes_utiles) >= 3:
            lignes_valides.append(ligne)

    if len(lignes_valides) < 3:
        return ""

    entete = "".join(f"<th>{html.escape(col)}</th>" for col in colonnes_utiles)
    corps = "".join(
        "<tr>"
        + "".join(f"<td>{html.escape(ligne[col])}</td>" for col in colonnes_utiles)
        + "</tr>"
        for ligne in lignes_valides
    )
    return f"<table><thead><tr>{entete}</tr></thead><tbody>{corps}</tbody></table>"


def _segmenter_materiaux(valeur: str) -> List[str]:
    """Découpe une valeur de matériau en éléments exploitables."""

    if not valeur:
        return []
    morceaux = _MATIERE_SEPARATORS.split(valeur)
    propres = []
    for morceau in morceaux:
        propre = morceau.strip()
        if propre:
            propres.append(propre.capitalize())
    return propres


def _aggreger_donnees_structurees(
    extraits: Sequence[SerpExtract],
) -> Tuple[List[Dict[str, str]], List[str]]:
    """Fusionne les données structurées et calcule des insights quantitatifs."""

    lignes: List[Dict[str, str]] = []
    valeurs_prix: List[float] = []
    materiaux = Counter()
    dimensions: List[str] = []

    for extrait in extraits:
        if not extrait.structured_data:
            continue
        for ligne in extrait.structured_data:
            lignes.append(ligne)
            prix_brut = ligne.get("Prix", "")
            if prix_brut:
                match = _PRICE_VALUE_REGEX.search(prix_brut)
                if match:
                    try:
                        valeurs_prix.append(float(match.group(1).replace(" ", "").replace(",", ".")))
                    except ValueError:
                        pass
            matiere = ligne.get("Matière") or ""
            for mat in _segmenter_materiaux(matiere):
                materiaux[mat] += 1
            dimension = ligne.get("Dimensions") or ""
            if dimension and dimension not in dimensions:
                dimensions.append(dimension)

    insights: List[str] = []
    if valeurs_prix:
        moyenne = statistics.mean(valeurs_prix)
        minimum = min(valeurs_prix)
        maximum = max(valeurs_prix)
        if len(valeurs_prix) > 1:
            dispersion = statistics.pstdev(valeurs_prix)
            insights.append(
                f"Prix moyen relevé : {moyenne:.0f} € (min {minimum:.0f} €, max {maximum:.0f} €)"
            )
            if dispersion:
                insights.append(f"Variation observée : ±{dispersion:.0f} € autour de la moyenne")
        else:
            insights.append(f"Prix observé : {moyenne:.0f} €")
    if materiaux:
        principaux = materiaux.most_common(2)
        resume = ", ".join(f"{nom} ({count})" for nom, count in principaux)
        insights.append(f"Matériaux dominants : {resume}")
    if dimensions:
        insights.append(f"Dimensions typiques : {', '.join(dimensions[:3])}")

    lignes_uniques: List[Dict[str, str]] = []
    deja_vus = set()
    for ligne in lignes:
        cle = (
            ligne.get("Produit", ""),
            ligne.get("Prix", ""),
            ligne.get("Dimensions", ""),
            ligne.get("Matière", ""),
            ligne.get("Caractéristiques", ""),
        )
        if cle in deja_vus:
            continue
        deja_vus.add(cle)
        lignes_uniques.append(ligne)

    return lignes_uniques[:20], insights[:5]


def allocate_word_budget(total: int = 1200, k: int = 10, weights: Optional[Sequence[float]] = None) -> List[int]:
    """Répartit un budget de mots selon des poids relatifs."""

    if k <= 0:
        return []
    if weights is None or not any(w > 0 for w in weights):
        weights = [1.0] * k
    poids = np.asarray(weights, dtype=float) if np is not None else None
    if poids is not None:
        poids = np.maximum(poids, 1e-6)
        poids = poids / poids.sum()
        allocations = [int(total * float(p)) for p in poids]
    else:
        base = total // k
        allocations = [base] * k
    diff = total - sum(allocations)
    for i in range(abs(diff)):
        index = i % k
        allocations[index] += 1 if diff > 0 else -1
    return allocations


def construire_plan_piliers(piliers: Sequence[DomaineSemantique], total: int) -> List[dict]:
    """Construit le plan détaillé des piliers avec budgets de mots et checklists."""

    if not piliers:
        return []

    poids = [max(pilier.poids, 1.0) for pilier in piliers]
    budgets = allocate_word_budget(total=total, k=len(piliers), weights=poids)

    plan = []
    for pilier, budget in zip(piliers, budgets):
        plan.append(
            {
                "titre": pilier.titre,
                "word_budget": budget,
                "sous_themes": pilier.sous_themes[:5],
                "checklist": pilier.checklist[:12],
                "faits": pilier.faits[:6],
                "questions": pilier.questions[:4],
            }
        )
        pilier.word_budget = budget
    return plan


def _plain_text(html_str: str) -> str:
    """Nettoie le HTML pour ne garder que le texte utile."""

    if not html_str:
        return ""
    txt = re.sub(r"<(script|style)[^>]*>.*?</\1>", " ", html_str, flags=re.S | re.I)
    txt = re.sub(r"<[^>]+>", " ", txt)
    txt = re.sub(r"\s+", " ", txt)
    return txt.strip()


def _remove_banned_phrases(text: str, bans: Sequence[str]) -> str:
    for phrase in bans:
        text = re.sub(re.escape(phrase), "", text, flags=re.IGNORECASE)
    return text


def _anti_promo(texte: str) -> str:
    return re.sub(r"(Notre collection|nos modèles|offres spéciales)", "", texte, flags=re.IGNORECASE)


def _tokeniser(texte: str) -> List[str]:
    return re.findall(r"[\wÀ-ÖØ-öø-ÿ'-]+", texte.lower())


def _phrases(texte: str) -> List[str]:
    return [p.strip() for p in re.split(r"(?<=[.!?])\s+", texte) if p.strip()]


def _dedensifier(texte: str) -> str:
    tokens = _tokeniser(_plain_text(texte))
    total = len(tokens) or 1
    freq = Counter(tokens)
    for terme, count in freq.items():
        if count / total > 0.025:
            pattern = re.compile(rf"\b{re.escape(terme)}\b", flags=re.IGNORECASE)
            remplacement = "modéré"
            quota = max(1, count // 4)
            texte = pattern.sub(remplacement, texte, count=quota)
    return texte


def post_editer_contenu(contenu: str, bans: Sequence[str]) -> str:
    """Post-traite le contenu généré pour respecter la charte éditoriale."""

    if not contenu:
        return ""
    texte = _remove_banned_phrases(contenu, bans)
    texte = _anti_promo(texte)
    # texte = _dedensifier(texte)  # ← DÉSACTIVÉ - causait des remplacements abusifs
    texte = re.sub(r"<li>\s*</li>", "", texte)
    texte = re.sub(r"\s{2,}", " ", texte)
    texte = re.sub(r"\n{3,}", "\n\n", texte)
    return texte.strip()


def calculer_metrics(texte: str, vocab_expected: Sequence[str]) -> dict:
    """Calcule quelques métriques de qualité sur le texte final."""

    tokens = _tokeniser(texte)
    total_tokens = len(tokens) or 1
    vocab = {v.lower() for v in vocab_expected if v}
    coverage = (
        len([t for t in vocab if t in tokens]) / len(vocab)
        if vocab
        else 0.0
    )

    phrases = _phrases(texte)
    phrases_factual = [p for p in phrases if re.search(r"\d", p) or re.search(r"selon|conforme|norme", p.lower())]
    specificity = len(phrases_factual) / (len(phrases) or 1)

    ngrams = [" ".join(tokens[i : i + 3]) for i in range(len(tokens) - 2)]
    compteur = Counter(ngrams)
    repetitions = sum(count for count in compteur.values() if count > 1)
    redundancy = repetitions / (len(ngrams) or 1)

    mots_par_phrase = [len(_tokeniser(p)) for p in phrases] or [0]
    moyenne = statistics.mean(mots_par_phrase)
    ecart = statistics.pstdev(mots_par_phrase) if len(mots_par_phrase) > 1 else 0.0
    readability = max(0.0, min(100.0, 207 - 1.015 * moyenne - 84.6 * (total_tokens / (len(phrases) or 1)) / 25))

    return {
        "coverage": float(round(coverage, 3)),
        "specificity": float(round(specificity, 3)),
        "redundancy": float(round(redundancy, 3)),
        "readability": float(round(readability, 1)),
        "avg_sentence_length": float(round(moyenne, 1)),
        "sentence_variance": float(round(ecart, 1)),
    }


def vocabulaire_piliers(piliers: Sequence[DomaineSemantique]) -> List[str]:
    vocab: List[str] = []
    for pilier in piliers:
        for terme in pilier.termes_dominants[:12] + pilier.entites[:8]:
            if terme and terme.lower() not in {v.lower() for v in vocab}:
                vocab.append(terme)
    return vocab


def extraire_termes_obligatoires(analyse: Optional[AnalyseSerp]) -> List[str]:
    if not analyse or not analyse.domaines:
        return []
    obligations: List[str] = []
    for domaine in analyse.domaines[:10]:
        obligations.extend(domaine.termes_dominants[:6])
        obligations.extend(domaine.entites[:4])
    resultats: List[str] = []
    vus: set[str] = set()
    for terme in obligations:
        if not terme:
            continue
        identifiant = terme.lower()
        if identifiant in vus:
            continue
        vus.add(identifiant)
        resultats.append(terme)
        if len(resultats) >= 60:
            break
    return resultats


def _purger_tables_vides(html: str) -> str:
    if not html:
        return ""

    def _score_cell(cell: str) -> int:
        contenu = re.sub(r"<[^>]+>", " ", cell)
        return 1 if re.search(r"\d|cm|mm|€|g/m²|%|°", contenu) else 0

    tables = re.findall(r"(<table>.*?</table>)", html, flags=re.S)
    for table in tables:
        cellules = re.findall(r"<t[hd][^>]*>(.*?)</t[hd]>", table, flags=re.S)
        if not cellules:
            continue
        score = sum(_score_cell(cell) for cell in cellules)
        if score / len(cellules) < 0.5:
            html = html.replace(table, "")
    return html


def enrichir_h2(contenu_html: str, termes: Sequence[str]) -> str:
    if not contenu_html:
        return ""
    base = [t for t in termes if t]
    if not base:
        return contenu_html
    expressions = [t for t in base if " " in t][:15] or base[:15]
    if not expressions:
        return contenu_html
    reste = expressions.copy()
    def _prochaines() -> str:
        nonlocal reste
        if not reste:
            return ""
        ajout = ", ".join(reste[:2])
        reste = reste[2:]
        return ajout

    matches = re.findall(r"(<h2>)(.*?)(</h2>)", contenu_html, flags=re.S)
    resultat = contenu_html
    for match in matches:
        debut, titre, fin = match
        if any(term.lower() in titre.lower() for term in expressions):
            continue
        ajout = _prochaines()
        if not ajout:
            break
        nouveau = f"{debut}{titre} — {ajout}{fin}"
        resultat = resultat.replace("".join(match), nouveau, 1)
    return resultat


def fusionner(original: str, patch: str) -> str:
    if not patch.strip():
        return original
    try:
        sections = re.findall(r'(<section class="pilier">.*?</section>)', patch, flags=re.S)
        if not sections:
            if "<div class=\"conclusion\">" in original:
                return re.sub(
                    r'(<div class="conclusion">)',
                    patch + "\n" + r"\1",
                    original,
                    count=1,
                    flags=re.S,
                )
            return original + "\n" + patch
        resultat = original
        for section in sections:
            match = re.search(r"<h2>(.*?)</h2>", section, flags=re.S)
            if not match:
                continue
            titre = re.escape(match.group(1).strip())
            pattern = rf'(<section class="pilier">\s*<h2>\s*{titre}\s*</h2>.*?)(</section>)'
            if re.search(pattern, resultat, flags=re.S):
                resultat = re.sub(pattern, r"\1\n" + section + r"\2", resultat, count=1, flags=re.S)
            else:
                resultat += "\n" + section
        return resultat
    except Exception:
        return original + "\n" + patch


def _telecharger_contenu_url(url: str, timeout: int = 10) -> str:
    """Télécharge le contenu HTML avec `requests` et une logique de retry."""

    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    ]

    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=("GET",),
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    try:
        response = session.get(
            url,
            headers={
                "User-Agent": user_agents[hash(url) % len(user_agents)],
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Cache-Control": "max-age=0",
            },
            timeout=timeout,
            allow_redirects=True,
        )
        response.raise_for_status()

        content_type = response.headers.get("Content-Type", "")
        if "text/html" not in content_type.lower():
            return ""

        return response.text
    except requests.exceptions.RequestException:
        return ""
    finally:
        session.close()


def _nettoyer_texte_semantique(texte: str) -> str:
    """Nettoie le texte pour l'analyse sémantique avancée."""

    if not texte:
        return ""

    texte = unicodedata.normalize("NFKC", texte)
    texte = re.sub(r"http[s]?://\S+", " ", texte)
    texte = re.sub(r"[\r\n\t]+", " ", texte)
    texte = re.sub(r"[^0-9A-Za-zÀ-ÖØ-öø-ÿ\s-]", " ", texte)
    texte = re.sub(r"\s+", " ", texte)
    return texte.strip().lower()


def _limiter_longueur(texte: str, max_caracteres: int = 8000) -> str:
    """Évite de dépasser une taille excessive pour chaque contenu source."""

    if len(texte) <= max_caracteres:
        return texte
    return texte[:max_caracteres] + " …"


def _extraire_termes_document(tfidf_ligne, termes: Sequence[str], top_n: int = 12) -> List[str]:
    """Récupère les termes dominants d'un document donné."""

    if np is None:
        return []

    vecteur = np.asarray(tfidf_ligne.toarray()).ravel()
    if vecteur.size == 0:
        return []
    indices = np.argsort(vecteur)[::-1]
    resultat: List[str] = []
    for indice in indices:
        if vecteur[indice] <= 0:
            continue
        terme = termes[indice]
        if terme not in _STOPWORDS_FR:
            resultat.append(terme)
        if len(resultat) >= top_n:
            break
    return resultat


def _construire_analyse_semantique_avancee(
    extraits: Sequence[SerpExtract],
    *,
    questions_paa: Optional[Sequence[str]] = None,
    suggestions: Optional[Sequence[str]] = None,
) -> AnalyseSerp:
    """Construit l'analyse sémantique approfondie à partir des extraits SERP."""

    questions_paa = [q.strip() for q in (questions_paa or []) if q]
    suggestions = [s.strip() for s in (suggestions or []) if s]

    sections: List[SectionSemantique] = []
    index_extrait: List[int] = []
    for idx, extrait in enumerate(extraits):
        sections_extrait = _scinder_sections(extrait)
        if not sections_extrait:
            texte_brut = extrait.texte_integral or extrait.extrait_contenu or extrait.snippet
            if texte_brut:
                sections_extrait = [
                    SectionSemantique(
                        titre=extrait.titre,
                        texte=texte_brut,
                        source_index=0,
                        url=extrait.url,
                        entites=_extraire_entites(texte_brut),
                        faits=_extraire_faits(texte_brut),
                        questions=_extraire_questions(texte_brut),
                    )
                ]
        for section in sections_extrait:
            sections.append(section)
            index_extrait.append(idx)

    sections_propres: List[SectionSemantique] = []
    index_extrait_propres: List[int] = []
    documents: List[str] = []
    for section, idx_map in zip(sections, index_extrait):
        propre = _nettoyer_texte_semantique(section.texte)
        if not propre:
            continue
        sections_propres.append(section)
        index_extrait_propres.append(idx_map)
        documents.append(propre)

    sections = sections_propres
    index_extrait = index_extrait_propres

    if not documents or TfidfVectorizer is None or np is None:
        lignes_simples = [
            "Synthèse SERP indisponible : bibliothèques d'analyse avancée manquantes.",
            "Contenu brut accessible via l'onglet d'inspection dans le fichier HTML généré.",
        ]
        return AnalyseSerp(
            resume="\n".join(lignes_simples),
            domaines=[],
            sources=[
                {
                    "titre": extrait.titre,
                    "url": extrait.url,
                    "snippet": extrait.snippet,
                    "texte_integral": _limiter_longueur(extrait.texte_integral),
                    "termes_dominants": [],
                }
                for extrait in extraits
            ],
            texte_prompt="\n".join(lignes_simples),
            questions_paa=questions_paa,
            suggestions=suggestions,
            intent_label=_detecter_intention(extraits[0].titre if extraits else ""),
        )

    try:
        vectorizer = TfidfVectorizer(
            stop_words=_STOPWORDS_FR,
            max_df=0.9,
            min_df=2,
            ngram_range=(1, 3),
        )
        matrice = vectorizer.fit_transform(documents)
    except ValueError:
        vectorizer = TfidfVectorizer(
            stop_words=_STOPWORDS_FR,
            max_df=0.95,
            min_df=1,
            ngram_range=(1, 2),
        )
        matrice = vectorizer.fit_transform(documents)

    termes = vectorizer.get_feature_names_out()

    embeddings = _embed_sections(documents)
    if embeddings is None and np is not None:
        embeddings = np.asarray(matrice.toarray())
        if embeddings.size:
            normes = np.linalg.norm(embeddings, axis=1, keepdims=True)
            embeddings = embeddings / np.maximum(normes, 1e-9)

    if embeddings is not None:
        indices_selectionnes = _selectionner_piliers_mmr(embeddings, k=10)
    else:
        indices_selectionnes = list(range(min(10, len(documents))))

    domaines: List[DomaineSemantique] = []
    for position, indice_section in enumerate(indices_selectionnes):
        section = sections[indice_section]
        tfidf_ligne = matrice[indice_section]
        termes_dominants = _extraire_termes_document(tfidf_ligne, termes, top_n=18)
        sous_themes = [t for t in termes_dominants if " " in t][:6]
        if not sous_themes:
            sous_themes = termes_dominants[:6]

        extrait_associe = extraits[index_extrait[indice_section]]
        resume = textwrap.shorten(section.texte, width=480, placeholder=" …")
        poids = float(np.asarray(tfidf_ligne.sum()).ravel()[0]) if np is not None else len(section.texte)
        table_data = list(_generer_lignes_table(section))
        if extrait_associe.structured_data:
            table_data.extend(extrait_associe.structured_data[:6])

        checklist = []
        for element in termes_dominants[:8] + section.entites[:8]:
            if element not in checklist:
                checklist.append(element)
        for fait in section.faits[:4]:
            if fait not in checklist:
                checklist.append(fait)

        questions = list(dict.fromkeys(section.questions + questions_paa))[:6]

        domaines.append(
            DomaineSemantique(
                titre=section.titre or f"Pilier {position + 1}",
                termes_dominants=termes_dominants,
                sous_themes=sous_themes,
                resume=resume,
                poids=poids,
                urls=[extrait_associe.url],
                entites=section.entites,
                faits=section.faits,
                questions=questions,
                checklist=checklist,
                table_rows=table_data,
            )
        )

    domaines.sort(key=lambda d: d.poids, reverse=True)

    sources = []
    for section, idx_extrait in zip(sections, index_extrait):
        extrait = extraits[idx_extrait]
        sources.append(
            {
                "titre": extrait.titre,
                "url": extrait.url,
                "snippet": extrait.snippet,
                "texte_integral": _limiter_longueur(extrait.texte_integral),
                "termes_dominants": section.entites[:10] + section.faits[:5],
            }
        )

    lignes_resume = ["Analyse avancée des contenus concurrents (SERP Google.fr)"]
    for index, domaine in enumerate(domaines, start=1):
        resume_bloc = textwrap.dedent(
            f"""
            {index}. {domaine.titre}
               Termes dominants : {', '.join(domaine.termes_dominants[:12])}
               Sous-thèmes majeurs : {', '.join(domaine.sous_themes[:6])}
               Entités clés : {', '.join(domaine.entites[:6])}
               Questions associées : {', '.join(domaine.questions[:3])}
               Extraits : {domaine.resume}
            """
        ).strip()
        lignes_resume.append(resume_bloc)

    if not domaines:
        lignes_resume.append("Analyse limitée : corpus insuffisant pour définir des piliers distincts.")

    resume = "\n".join(lignes_resume)

    donnees_structurees, insights_structurels = _aggreger_donnees_structurees(extraits)

    longueur_cible = max(int(CONFIG.get("target_words_min", 1200)), 1200)
    plan = construire_plan_piliers(domaines, total=longueur_cible)

    return AnalyseSerp(
        resume=resume,
        domaines=domaines,
        sources=sources,
        texte_prompt=resume,
        plan_sections=plan,
        questions_paa=questions_paa,
        suggestions=suggestions,
        intent_label=_detecter_intention(extraits[0].titre if extraits else ""),
        donnees_structurees=donnees_structurees,
        insights_structurels=insights_structurels,
    )


def _erreur_quota_depasse(erreur: Optional[dict]) -> bool:
    """Détecte si la réponse d'erreur indique un dépassement de quota."""

    if not erreur:
        return False

    message = erreur.get("message", "")
    if isinstance(message, str):
        message_min = message.lower()
        if any(mot in message_min for mot in ("quota", "limit", "exceeded")):
            return True

    for detail in erreur.get("errors", []):
        raison = (detail or {}).get("reason", "")
        if raison in {"dailyLimitExceeded", "quotaExceeded", "userRateLimitExceeded"}:
            return True
    return False


def _executer_requete_custom_search(params: dict) -> dict:
    """Appelle l'API Google Custom Search et gère les erreurs courantes."""

    url = f"{API_URL}?{urllib.parse.urlencode(params)}"
    try:
        with urllib.request.urlopen(url, timeout=15) as reponse:  # noqa: S310
            contenu = reponse.read().decode("utf-8")
    except urllib.error.HTTPError as exc:  # noqa: PERF203
        corps = exc.read().decode("utf-8", errors="ignore")
        try:
            donnees = json.loads(corps) if corps else {}
        except json.JSONDecodeError:
            donnees = {"error": {"message": corps}}

        erreur = donnees.get("error") if isinstance(donnees, dict) else None
        if exc.code in {403, 429} and _erreur_quota_depasse(erreur):
            raise _QuotaExceededError(erreur.get("message", "Quota dépassé")) from exc
        message = erreur.get("message") if isinstance(erreur, dict) else None
        raise _ApiRequestError(message or f"Erreur HTTP {exc.code}") from exc
    except urllib.error.URLError as exc:
        raise _ApiRequestError(str(exc)) from exc

    try:
        donnees = json.loads(contenu)
    except json.JSONDecodeError as exc:  # pragma: no cover - erreurs rares
        raise _ApiRequestError("Réponse JSON invalide") from exc

    erreur = donnees.get("error") if isinstance(donnees, dict) else None
    if isinstance(erreur, dict) and erreur:
        if _erreur_quota_depasse(erreur):
            raise _QuotaExceededError(erreur.get("message", "Quota dépassé"))
        raise _ApiRequestError(erreur.get("message", "Erreur API inconnue"))

    return donnees


def _collecter_resultats_serp(
    requete: str,
    api_key_personnalisee: Optional[str] = None,
    nb_resultats: int = 50,
) -> Tuple[List[SerpExtract], Dict[str, List[str]]]:
    """Récupère les résultats et extraits de la SERP via Google Custom Search."""

    if nb_resultats <= 0:
        return []

    print(
        f"\n🔍 Collecte des résultats Google pour \"{requete}\" "
        f"(objectif : {nb_resultats} extraits)"
    )

    gestionnaire = _ApiKeyManager(
        ([api_key_personnalisee] if api_key_personnalisee else []) + API_KEYS
    )
    resultats: List[SerpExtract] = []
    questions_paa: List[str] = []
    suggestions: List[str] = []
    start_index = 1

    while len(resultats) < nb_resultats:
        cle_api = gestionnaire.prochaine_cle()
        if not cle_api:
            break

        params = {
            "key": cle_api,
            "cx": CX_ID,
            "q": requete,
            "num": min(10, nb_resultats - len(resultats)),
            "start": start_index,
            "gl": "fr",
            "lr": "lang_fr",
        }

        print(f"   → Requête Google Custom Search (index de départ : {start_index})")

        try:
            donnees = _executer_requete_custom_search(params)
        except _QuotaExceededError:
            gestionnaire.mettre_en_pause(cle_api)
            suffixe = cle_api[-6:]
            print(f"⏳ Clé API en pause (quota atteint) : …{suffixe}")
            continue
        except _ApiRequestError:
            print("   ⚠️ Erreur lors de la requête API, nouvelle tentative avec une autre clé…")
            time.sleep(1)
            continue

        items = donnees.get("items") or []
        if not items:
            break

        print(f"   → {len(items)} résultats reçus, analyse en cours…")

        for item in items:
            if len(resultats) >= nb_resultats:
                break

            url_page = item.get("link") or ""
            titre = item.get("title") or ""
            snippet = item.get("snippet") or item.get("htmlSnippet") or ""
            if not url_page or not titre:
                continue

            domaine = urllib.parse.urlparse(url_page).netloc or url_page
            print(
                "      ▸ Analyse du résultat "
                f"#{len(resultats) + 1} : {domaine}"
            )

            contenu_html = _telecharger_contenu_url(url_page)
            time.sleep(random.uniform(0.3, 1.0))
            texte_page = ""
            if contenu_html:
                if trafilatura is not None:
                    try:
                        texte_page = trafilatura.extract(
                            contenu_html,
                            include_links=False,
                            include_tables=True,
                        ) or ""
                    except Exception:  # noqa: BLE001
                        texte_page = ""
                if not texte_page:
                    texte_page = _extraire_texte_html(contenu_html)
            contenu_source = texte_page or snippet or ""
            extrait = _tronquer_texte(contenu_source, max_mots=120)
            texte_integral = _limiter_longueur(contenu_source)

            if contenu_html:
                print("        ✓ Contenu téléchargé, extraction du texte…")
            else:
                print("        ⚠️ Impossible de récupérer le contenu de la page, utilisation du snippet uniquement")

            pagemap = item.get("pagemap") or {}
            donnees_structurees = _extraire_donnees_structurees(pagemap)

            resultats.append(
                SerpExtract(
                    titre=titre.strip(),
                    url=url_page,
                    snippet=_tronquer_texte(snippet or "", max_mots=40),
                    extrait_contenu=extrait,
                    texte_integral=texte_integral,
                    structured_data=donnees_structurees,
                )
            )
            for bloc in pagemap.get("question", []) + pagemap.get("faqquestion", []):
                question = (bloc or {}).get("name") or (bloc or {}).get("text")
                if question and question not in questions_paa:
                    questions_paa.append(question.strip())
            related = pagemap.get("listitem") or []
            for suggestion in related:
                nom = (suggestion or {}).get("name")
                if nom and nom not in suggestions:
                    suggestions.append(nom.strip())

        queries = donnees.get("queries", {}) if isinstance(donnees, dict) else {}
        next_pages = queries.get("nextPage") if isinstance(queries, dict) else None
        if next_pages:
            start_index = next_pages[0].get("startIndex", start_index + len(items))
        else:
            start_index += len(items)

        related = donnees.get("relatedSearches") if isinstance(donnees, dict) else None
        if isinstance(related, list):
            for suggestion in related:
                terme = (suggestion or {}).get("query")
                if terme and terme not in suggestions:
                    suggestions.append(terme.strip())

        time.sleep(1)

    return resultats, {"paa": questions_paa, "suggestions": suggestions}


def collecter_semantique_serp(
    requete: CollectionRequest,
    serp_api_key: Optional[str],
    nb_resultats: int = 50,
) -> Optional[AnalyseSerp]:
    """Construit une analyse sémantique détaillée à partir de la SERP."""

    if nb_resultats <= 0:
        return None

    extraits, extras = _collecter_resultats_serp(
        requete.requete_serp,
        api_key_personnalisee=serp_api_key,
        nb_resultats=nb_resultats,
    )
    if not extraits:
        print("⚠️ Aucun extrait SERP n'a pu être collecté")
        return None

    print(f"📚 Corpus sémantique constitué à partir de {len(extraits)} sources")

    analyse = _construire_analyse_semantique_avancee(
        extraits,
        questions_paa=extras.get("paa", []),
        suggestions=extras.get("suggestions", []),
    )
    if analyse.domaines:
        print(
            f"🧭 Domaines piliers identifiés : {len(analyse.domaines)} "
            "(pondération basée sur la densité TF-IDF)"
        )
    else:
        print("ℹ️ Analyse avancée limitée : peu de signal exploitable ou dépendances manquantes")

    return analyse


def _construire_prompt(
    requete: CollectionRequest, analyse_serp: Optional[AnalyseSerp] = None
) -> str:
    """Assemble le prompt détaillé envoyé à l'IA."""

    mots_secondaires = (
        ", ".join(requete.mots_cles_secondaires)
        if requete.mots_cles_secondaires
        else "à déterminer selon le contexte"
    )
    liens = (
        "\n- Liens internes suggérés : " + ", ".join(requete.liens_internes)
        if requete.liens_internes
        else ""
    )

    termes_obligatoires = extraire_termes_obligatoires(analyse_serp)

    if analyse_serp and analyse_serp.domaines:
        plan_piliers = []
        for index, domaine in enumerate(analyse_serp.domaines, start=1):
            checklist = ", ".join(domaine.checklist[:8]) or "-"
            faits = ", ".join(domaine.faits[:3]) or "-"
            questions = ", ".join(domaine.questions[:3]) or "-"
            table_note = (
                "; ".join(
                    " | ".join(
                        filtre
                        for filtre in (
                            ligne.get("Produit") or ligne.get("Type"),
                            ligne.get("Prix"),
                            ligne.get("Dimensions"),
                            ligne.get("Matière"),
                        )
                        if filtre
                    )
                    for ligne in domaine.table_rows[:3]
                )
                if domaine.table_rows
                else "aucune donnée structurée"
            )
            plan_piliers.append(
                textwrap.dedent(
                    f"""
                    {index}. {domaine.titre} — budget {domaine.word_budget} mots
                       Sous-thèmes : {', '.join(domaine.sous_themes[:5])}
                       Checklist terminologique : {checklist}
                       Faits/contraintes : {faits}
                       Questions à adresser : {questions}
                       Base tableau : {table_note}
                    """
                ).strip()
            )
        suggestions = ", ".join(analyse_serp.suggestions[:6]) if analyse_serp.suggestions else ""
        contexte = "\n\nSYNTHÈSE SERP (MMR + embeddings) :\n" + "\n".join(plan_piliers)
        if suggestions:
            contexte += f"\nRequêtes associées à intégrer discrètement : {suggestions}"
    elif analyse_serp:
        contexte = (
            "\n\nSYNTHÈSE SERP :\n"
            + (analyse_serp.resume or "Peu de signal exploitable.")
        )
    else:
        contexte = ""

    bloc_insights_lignes: List[str] = []
    if analyse_serp:
        if analyse_serp.insights_structurels:
            bloc_insights_lignes.append(
                "Insights quantitatifs (schema.org) : "
                + " | ".join(analyse_serp.insights_structurels[:4])
            )
        if analyse_serp.donnees_structurees:
            extraits_schema = []
            for ligne in analyse_serp.donnees_structurees[:5]:
                elements = [
                    ligne.get("Produit", ""),
                    ligne.get("Prix", ""),
                    ligne.get("Dimensions", ""),
                    ligne.get("Matière", ""),
                ]
                extraits_schema.append(" | ".join([elt for elt in elements if elt]))
            extraits_schema = [val for val in extraits_schema if val]
            if extraits_schema:
                bloc_insights_lignes.append(
                    "Échantillon schema.org : " + "; ".join(extraits_schema)
                )

    bloc_insights = "\n".join(bloc_insights_lignes)
    if bloc_insights:
        contexte = (contexte + "\n" if contexte else "") + bloc_insights

    informations_supplementaires = (
        f"INFORMATIONS SUPPLÉMENTAIRES : {requete.infos_supplementaires}\n"
        if requete.infos_supplementaires
        else ""
    )

    intention_detectee = (
        analyse_serp.intent_label if analyse_serp else _detecter_intention(requete.titre_collection)
    )
    intention_cible = "transac"
    banned = ", ".join(CONFIG.get("ban_phrases", []))
    tone = CONFIG.get("tone", "pédagogique_expert_non_promo")
    longueur_cible = max(int(CONFIG.get("target_words_min", 1200)), 1200)
    variance = float(CONFIG.get("word_count_variance", 0.1))
    min_mots = int(longueur_cible * (1 - variance))
    max_mots = int(longueur_cible * (1 + variance))

    faq_data = ""
    if analyse_serp and analyse_serp.questions_paa:
        faq_data = (
            "\nFAQ DATA-DRIVEN : prioriser ces questions issues de la SERP : "
            + "; ".join(analyse_serp.questions_paa[:8])
        )

    bloc_obligatoires = ""
    if termes_obligatoires:
        bloc_obligatoires = (
            "\nTERMES OBLIGATOIRES À INTÉGRER DANS LES PHRASES (pas en liste seule) : "
            + ", ".join(termes_obligatoires)
            + "\n> Chaque terme doit apparaître ≥ 1 fois, les 15 premiers ≥ 2 fois, de façon naturelle, répartis dans les sections."
        )

    bloc_caracteristiques = _bloc_caracteristiques_prompt(
        requete.caracteristiques,
        focus_unique=requete.focus_unique,
    )

    bloc_differenciation = (
        "\n" + requete.guidelines_differenciation
        if requete.guidelines_differenciation
        else ""
    )

    return f"""Tu es un rédacteur web stratégique, expert du SEO e-commerce et de la sémantique avancée pour le marché français, et tu écris avant tout pour des clients en quête d'inspiration.
Ton style reste chaleureux, immersif et agréable à lire : chaque information doit couler naturellement, même lorsqu'elle est technique.

OBJECTIF : produire le meilleur contenu de référence sur la collection "{requete.titre_collection}" en surpassant la SERP actuelle.
Intention détectée : {intention_detectee} | Intention cible : {intention_cible}

CONTRAINTES SEMANTIQUES :
- Mot-clé principal : {requete.mots_cles_principaux[0]}
- Mots-clés secondaires prioritaires : {mots_secondaires}
- Exploiter les champs lexicaux, entités et arguments observés dans la SERP.
- Tonalité imposée : {tone}.
- Phrases interdites : {banned}.
{('- Focus unique à marteler : ' + (requete.focus_unique or requete.titre_collection) + '.')}
{bloc_obligatoires}
{('\n' + bloc_caracteristiques) if bloc_caracteristiques else ''}
{bloc_differenciation}

STRUCTURE ATTENDUE DU CONTENU HTML ({min_mots} à {max_mots} mots) :
1. <div class="intro"> (120-150 mots)
   - Contexte marché + promesse de la collection.
   - Utiliser le mot-clé principal dès la première phrase.
   - Introduire brièvement les bénéfices majeurs et une image sensorielle (« Fini… », « Un confort visible dès le premier usage »).
   - Terminer par un micro-CTA avec verbe d'action (ex : « Découvrir la sélection », « Comparer les finitions »).
2. Une section par pilier sémantique (<section class="pilier">)
   - Chaque pilier commence par <h2>« [Titre du pilier] »</h2> intégrant une expression longue tirée des termes obligatoires.
   - Développer 230-300 mots par pilier en combinant paragraphes (<p>), listes (<ul>/<ol>) et encadrés (<blockquote> ou <div class="focus">) pour aérer la lecture.
   - Introduire des sous-parties avec <h3> selon les sous-thèmes fournis.
   - Comparer, expliquer, donner des critères concrets, intégrer des notions de prix, matériaux, usages, innovations.
   - Chaque paragraphe intègre un fait mesurable ou un critère tangible (chiffre, norme, usage précis).
   - Intégrer au moins un tableau <table> nourri de données tangibles (dimensions, matériaux, prix moyens) liées au focus unique.
   - Ajouter pour chaque pilier un bloc <details class="accordion"> avec <summary> pour approfondir un angle secondaire sans cannibaliser les autres pages.
   - Conclure chaque pilier par une liste <ul> d'arguments clés centrés sur {requete.focus_unique or 'la spécificité'} suivie d'un micro-CTA invitant à comparer ou explorer.
3. Section "Conseils d'expert" (<h2>)
   - 2-3 paragraphes courts orientés décision (120-150 mots).
   - Citer tests internes, retours clients et ratios calculés à partir des données structurées collectées.
   - Introduire un encadré « Astuce terrain » en 3 puces pour guider la décision.
4. Section "Données à surveiller" (<section class="insights">)
   - Exploiter les insights quantitatifs fournis (prix moyen, matériaux dominants, dimensions typiques) pour construire un tableau ou une liste chiffrée.
   - Mettre en avant les tendances à surveiller et préciser le prochain point de contrôle ou seuil d'alerte.
   - Terminer par un CTA discret (« Voir les modèles stables », « Comparer les matériaux »).
5. Section "Ce que peu de gens savent" (<div class="contrepoint">)
   - Format <h2> + <ul> de 3-4 points révélant informations surprenantes ou conseils rarement évoqués.
   - Mentionner au moins une anecdote issue des données clients ou d'un test terrain.
   - Conclure par une phrase sensorielle qui renforce la différenciation.
6. Section FAQ (<div class="faq">)
   - 6 à 8 questions dont : {', '.join(requete.questions_faq[:5])}.
   - Respecter le format :
     <div class="faq-item"> -> <h3 class="faq-question">Question</h3> + <div class="faq-answer">Réponse détaillée (80-110 mots)</div>
   - Chaque réponse intègre naturellement 2 à 3 termes obligatoires distincts.
   - Prioriser les questions issues de la SERP si pertinentes.{faq_data}
7. <div class="conclusion"> (80-100 mots)
   - Synthèse stratégique + appel à l'action fort.
   - Ajouter 2 à 3 liens internes pertinents (format : <a href="/chemin">Ancre riche</a>){liens}

GUIDELINES ÉDITORIALES :
- Style expert, pédagogique et accessible, riche en données concrètes (matériaux, usages, chiffres clés si génériques, comparaisons) tout en restant chaleureux et immersif.
- Chaque terme technique, norme ou matériau spécifique est immédiatement traduit en mots simples (entre parenthèses ou après une virgule) pour rester compréhensible.
- Marteler la spécificité dominante de la collection (ex : couleur ou matière) dès l'introduction, dans chaque titre de pilier et dans la conclusion pour éviter la cannibalisation avec les autres gammes.
- Les blocs <details> doivent dévoiler des scénarios d'usage ou conseils d'achat spécifiques à cette collection, distincts des autres pages.
- Varier les schémas syntaxiques : alternance phrases courtes / complexes, synonymes, champs lexicaux divers.
- Pondérer la répétition des termes selon leur importance (forte présence pour entités dominantes, modérée sinon).
- Bannir les formulations vagues ou auto-référentielles. Aucun contenu artificiel ou vide.
- HTML propre uniquement : pas de commentaires ni de code-block markdown. Respecter les balises demandées.
- Veiller à ce que chaque pilier ait une progression logique : contexte → développement → bénéfices → limites éventuelles.
- Intégrer subtilement les bénéfices business (service client, logistique, durabilité) sans ton promotionnel.
- Chaque paragraphe doit référencer au moins un fait ou une entité issue des checklists.
- Générer des tableaux uniquement lorsqu'au moins trois lignes disposent de valeurs concrètes.
- Chaque pilier se clôt sur un call-to-action discret renvoyant vers un segment de gamme cohérent via les liens internes fournis.
- Varier le vocabulaire en évitant toute répétition excessive (>2.5% du texte).
- Chaque paragraphe contient au moins un élément concret (mesure, critère mesurable, comparaison factuelle ou usage précis).
- Favoriser une lecture fluide : phrases de 12 à 24 mots en moyenne, transitions naturelles, paragraphes aérés.
- Introduire des micro-CTA dynamiques en fin d'introduction, de pilier, de section insights et avant la conclusion (verbes : Découvrez, Comparez, Profitez, Adoptez, Explorez).
- Insérer au moins une phrase commençant par « Fini » et une autre valorisant la durabilité (ex : « Une solution pensée pour durer »).
- La section « Données à surveiller » doit citer explicitement au moins deux chiffres issus des insights fournis (prix, matériaux, dimensions) et relier ces valeurs à un conseil actionnable.
- La section « Ce que peu de gens savent » doit rester factuelle, s'appuyer sur des signaux clients/tests et éviter toute formulation promotionnelle.
- Utiliser systématiquement listes, tableaux et encadrés visuels pour limiter les blocs continus à moins de 90 mots.
- Guider le lecteur vers la décision d'achat : critères de sélection, signaux de qualité, services logistiques ou SAV, preuves sociales génériques.

{informations_supplementaires}{contexte}

Produit le contenu HTML final immédiatement. Commence par <div class="intro"> et termine par </div> de la conclusion.
Assure-toi de dépasser 1100 mots tout en restant fluide et naturel.
"""


def generer_description_collection(
    api_key: str,
    titre_collection: str,
    mots_cles_principaux: Optional[Sequence[str]] = None,
    mots_cles_secondaires: Optional[Sequence[str]] = None,
    questions_faq: Optional[Sequence[str]] = None,
    infos_supplementaires: Optional[str] = None,
    liens_internes: Optional[Sequence[str]] = None,
    serp_api_key: Optional[str] = None,
    nb_resultats_serp: int = 50,
    requete_serp: Optional[str] = None,
):
    """
    Génère une description SEO optimisée pour une collection e-commerce

    Args:
        api_key: Ta clé API OpenAI (format: sk-proj-...)
        titre_collection: Nom de la collection (ex: "Vestes en Cuir Femme")
        mots_cles_principaux: Liste des mots-clés principaux (ex: ["veste cuir femme"])
        mots_cles_secondaires: Liste des mots-clés secondaires (ex: ["perfecto", "blouson"])
        questions_faq: Liste de questions pour la FAQ (ex: ["Comment choisir la taille ?"])
        infos_supplementaires: Infos sur tes produits (ex: "Cuir véritable, livraison gratuite")

    Returns:
        dict: Contenu HTML + métadonnées SEO
    """

    requete = CollectionRequest(
        titre_collection=titre_collection,
        mots_cles_principaux=list(mots_cles_principaux or []),
        mots_cles_secondaires=list(mots_cles_secondaires or []),
        questions_faq=list(questions_faq or []),
        infos_supplementaires=infos_supplementaires,
        liens_internes=list(liens_internes or []),
        requete_serp=requete_serp,
    )

    _preparer_differenciation_collections([requete])

    # Initialiser OpenAI
    client = OpenAI(api_key=api_key)

    analyse_serp = collecter_semantique_serp(
        requete,
        serp_api_key=serp_api_key,
        nb_resultats=nb_resultats_serp,
    )
    if analyse_serp:
        print("🌐 Contexte SERP collecté automatiquement")
        for question in analyse_serp.questions_paa:
            if question not in requete.questions_faq:
                requete.questions_faq.append(question)
        if len(requete.questions_faq) > 12:
            requete.questions_faq = requete.questions_faq[:12]
    else:
        print("⚠️ Aucun contexte SERP disponible (clé manquante ou collecte impossible)")

    # Construire le prompt
    print("🧠 Construction du prompt avec le contexte disponible…")
    prompt = _construire_prompt(requete, analyse_serp=analyse_serp)
    termes_obligatoires = extraire_termes_obligatoires(analyse_serp)

    print(f"\n🔄 Génération du contenu pour : {requete.titre_collection}")
    print(f"📝 Mots-clés : {', '.join(requete.mots_cles_principaux)}")

    system_message = {
        "role": "system",
        "content": (
            "Tu es un expert en rédaction SEO e-commerce pour le marché français. "
            "Tu produis du contenu HTML optimisé, unique et engageant."
        ),
    }

    try:
        print("🤖 Appel à l'API OpenAI en cours…")
        # Appel à l'API OpenAI
        response = client.chat.completions.create(
            model="gpt-5",
            messages=[
                system_message,
                {
                    "role": "user",
                    "content": prompt,
                },
            ],
        )

        contenu_html = response.choices[0].message.content.strip()

        # Nettoyer les éventuels marqueurs markdown
        contenu_html = contenu_html.replace("```html", "").replace("```", "").strip()
        contenu_html = _purger_tables_vides(contenu_html)
        if termes_obligatoires:
            contenu_html = enrichir_h2(contenu_html, termes_obligatoires)
        contenu_html = post_editer_contenu(contenu_html, CONFIG.get("ban_phrases", []))

        vocab_expected = vocabulaire_piliers(analyse_serp.domaines) if analyse_serp else []
        plain_text = _plain_text(contenu_html)
        nb_mots_reels = len(_tokeniser(plain_text))
        metrics = calculer_metrics(plain_text, vocab_expected)

        tentatives = 0
        max_reparations = 2
        while tentatives < max_reparations:
            longueur_ok = nb_mots_reels >= 1100
            couverture_ok = metrics["coverage"] >= 0.80 if vocab_expected else True
            specificite_ok = metrics["specificity"] >= 0.35
            if longueur_ok and couverture_ok and specificite_ok:
                break

            tentatives += 1
            termes_manquants = [
                terme
                for terme in termes_obligatoires
                if terme and terme.lower() not in plain_text.lower()
            ]
            prompt_repair = (
                prompt
                + """

RÉPARATION CIBLÉE :
- Couverture actuelle: {coverage}, Spécificité: {specificity}, Mots réels: {mots}
- Termes manquants: {manquants}
- Étoffe les sections où les termes obligatoires manquent ; ajoute des paragraphes concrets (faits chiffrés génériques, critères mesurables, comparaisons).
- Intègre explicitement les termes manquants dans des phrases naturelles ; n’ajoute PAS de liste « focus-termes ».
- Garde la structure existante, ajoute seulement du contenu pertinent.
""".format(
                    coverage=metrics["coverage"],
                    specificity=metrics["specificity"],
                    mots=nb_mots_reels,
                    manquants=", ".join(termes_manquants[:20]) or "aucun",
                )
            )

            rep = client.chat.completions.create(
                model="gpt-5",
                messages=[
                    system_message,
                    {
                        "role": "user",
                        "content": prompt_repair,
                    },
                ],
            )

            patch = rep.choices[0].message.content.strip()
            patch = patch.replace("```html", "").replace("```", "").strip()
            patch = _purger_tables_vides(patch)
            contenu_html = fusionner(contenu_html, patch)
            contenu_html = _purger_tables_vides(contenu_html)
            if termes_obligatoires:
                contenu_html = enrichir_h2(contenu_html, termes_obligatoires)
            contenu_html = post_editer_contenu(contenu_html, CONFIG.get("ban_phrases", []))

            plain_text = _plain_text(contenu_html)
            nb_mots_reels = len(_tokeniser(plain_text))
            metrics = calculer_metrics(plain_text, vocab_expected)

        # Dernière harmonisation
        contenu_html = _purger_tables_vides(contenu_html)
        if termes_obligatoires:
            contenu_html = enrichir_h2(contenu_html, termes_obligatoires)
        contenu_html = post_editer_contenu(contenu_html, CONFIG.get("ban_phrases", []))
        plain_text = _plain_text(contenu_html)
        nb_mots_reels = len(_tokeniser(plain_text))
        metrics = calculer_metrics(plain_text, vocab_expected)

        # Générer meta title
        meta_title = _generer_meta_title(
            requete.titre_collection,
            analyse_serp=analyse_serp,
            focus_unique=requete.focus_unique,
        )

        # Générer meta description
        meta_description = _generer_meta_description(
            requete.titre_collection,
            requete.mots_cles_principaux,
            requete.caracteristiques,
            analyse_serp=analyse_serp,
            focus_unique=requete.focus_unique,
        )

        # Préparer le résultat
        resultat = {
            "titre_collection": requete.titre_collection,
            "contenu_html": contenu_html,
            "mots_cles_principaux": requete.mots_cles_principaux,
            "mots_cles_secondaires": requete.mots_cles_secondaires,
            "date_generation": datetime.now().isoformat(),
            "modele_utilise": "gpt-5",
            "nombre_mots": nb_mots_reels,
            "meta_title": meta_title,
            "meta_description": meta_description,
            "questions_faq": requete.questions_faq,
            "liens_internes": requete.liens_internes,
            "requete_serp": requete.requete_serp,
            "contexte_serp": analyse_serp.resume if analyse_serp else "",
            "analyse_serp": analyse_serp.as_dict() if analyse_serp else None,
            "quality_metrics": metrics,
            "caracteristiques": requete.caracteristiques,
        }

        print("✅ Contenu généré avec succès!")
        print(f"📊 Nombre de mots réels : ~{resultat['nombre_mots']}")
        print(f"📈 Scores qualité : couverture={metrics['coverage']}, spécificité={metrics['specificity']}, redondance={metrics['redundancy']}")

        return resultat

    except Exception as e:
        print(f"❌ Erreur lors de la génération : {str(e)}")
        raise


def sauvegarder_html(resultat, dossier_sortie: str | os.PathLike[str] = "collections_generees"):
    """
    Sauvegarde le contenu généré dans un fichier HTML
    """
    # Créer le dossier si nécessaire
    os.makedirs(dossier_sortie, exist_ok=True)

    # Nom de fichier
    nom_fichier_base = resultat['titre_collection'].lower().replace(' ', '_').replace('/', '_')
    chemin_html = os.path.join(dossier_sortie, f"{nom_fichier_base}.html")

    analyse_serp = resultat.get('analyse_serp') or {}
    resume_serp = resultat.get('contexte_serp') or analyse_serp.get('resume') or ""

    contexte_serp_html = ""
    if resume_serp:
        contexte_serp_html = (
            "<details class=\"serp-context\">"
            "<summary>Analyse SERP agrégée</summary>"
            f"<pre>{html.escape(str(resume_serp))}</pre>"
            "</details>"
        )

    piliers_html = ""
    domaines = analyse_serp.get('domaines') or []
    if domaines:
        blocs = []
        for domaine in domaines:
            termes = ', '.join((domaine.get('termes_dominants') or [])[:10])
            sous_themes = ', '.join((domaine.get('sous_themes') or [])[:6])
            resume = domaine.get('resume') or ''
            urls = domaine.get('urls') or []
            table_rows = domaine.get('table_rows') or []
            table_html = make_table(table_rows) if table_rows else ''
            urls_html = ''.join(
                f"<li><a href=\"{html.escape(url)}\" target=\"_blank\" rel=\"nofollow noopener\">{html.escape(url)}</a></li>"
                for url in urls
            )
            blocs.append(
                "<article class=\"serp-pillar\">"
                f"<h3>{html.escape(domaine.get('titre', 'Pilier'))}</h3>"
                f"<p class=\"serp-pillar__resume\">{html.escape(str(resume))}</p>"
                f"<p><strong>Termes dominants :</strong> {html.escape(termes)}</p>"
                f"<p><strong>Sous-thèmes prioritaires :</strong> {html.escape(sous_themes)}</p>"
                f"<ul class=\"serp-pillar__urls\">{urls_html}</ul>"
                f"{table_html}"
                "</article>"
            )
        piliers_html = (
            "<section class=\"serp-pillars\">"
            "<h2>Domaines sémantiques priorisés</h2>"
            + "".join(blocs)
            + "</section>"
        )

    sources_html = ""
    sources = analyse_serp.get('sources') or []
    if sources:
        fiches = []
        for source in sources:
            termes = ', '.join((source.get('termes_dominants') or [])[:8])
            contenu_brut = source.get('texte_integral') or ''
            contenu_source = html.escape(contenu_brut)
            fiches.append(
                "<details class=\"serp-source\">"
                f"<summary>{html.escape(source.get('titre') or 'Source SERP')} — {html.escape(source.get('url') or '')}</summary>"
                f"<p><strong>Termes dominants :</strong> {html.escape(termes)}</p>"
                f"<p><strong>Snippet Google :</strong> {html.escape(source.get('snippet') or '')}</p>"
                f"<pre>{contenu_source}</pre>"
                "</details>"
            )
        sources_html = (
            "<section class=\"serp-sources\">"
            "<h2>Corpus SERP analysé</h2>"
            + "".join(fiches)
            + "</section>"
        )

    # Créer une page HTML complète
    html_complet = f"""<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{resultat['meta_title']}</title>
    <meta name="description" content="{resultat['meta_description']}">
    <style>
        body {{ font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; line-height: 1.6; }}
        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        h3 {{ color: #555; }}
        .intro {{ font-size: 1.1em; margin: 20px 0; padding: 15px; background: #ecf0f1; border-left: 4px solid #3498db; }}
        .conclusion {{ margin: 30px 0; padding: 15px; background: #e8f5e9; border-left: 4px solid #4caf50; }}
        ul {{ margin: 10px 0; padding-left: 25px; }}
        li {{ margin: 8px 0; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #3498db; color: white; }}
        tr:nth-child(even) {{ background-color: #f2f2f2; }}
        .faq {{ margin: 30px 0; }}
        .faq-item {{ margin: 15px 0; padding: 15px; background: #fff; border: 1px solid #ddd; border-radius: 5px; }}
        .faq-question {{ color: #2c3e50; margin: 0 0 10px 0; cursor: pointer; }}
        .faq-answer {{ color: #555; line-height: 1.7; }}
        a {{ color: #3498db; text-decoration: none; }}
        a:hover {{ text-decoration: underline; }}
        .meta-info {{ background: #fff3cd; padding: 15px; margin-bottom: 20px; border-radius: 5px; }}
        details.serp-context {{ background: #f6f8fa; padding: 15px; border-radius: 6px; margin: 20px 0; border: 1px solid #d0d7de; }}
        details.serp-context summary {{ cursor: pointer; font-weight: 600; color: #2c3e50; }}
        details.serp-context pre {{ white-space: pre-wrap; font-size: 0.9em; line-height: 1.5; }}
        .serp-pillars {{ background: #fdfefe; border: 1px solid #e0e6ed; padding: 20px; border-radius: 6px; margin: 20px 0; }}
        .serp-pillars h2 {{ margin-top: 0; }}
        .serp-pillar {{ border: 1px solid #e5e9f2; border-radius: 6px; padding: 15px; margin-bottom: 15px; background: #ffffff; }}
        .serp-pillar__resume {{ color: #445; font-style: italic; }}
        .serp-pillar__urls {{ margin: 10px 0 0 0; padding-left: 18px; }}
        .serp-sources {{ margin: 30px 0; }}
        .serp-sources h2 {{ margin-bottom: 15px; }}
        details.serp-source {{ border: 1px solid #e0e0e0; border-radius: 6px; margin-bottom: 10px; padding: 12px; background: #fafafa; }}
        details.serp-source summary {{ cursor: pointer; font-weight: 600; color: #2c3e50; }}
        details.serp-source pre {{ max-height: 240px; overflow-y: auto; white-space: pre-wrap; background: #fff; padding: 10px; border: 1px solid #eee; }}
    </style>
</head>
<body>
    <div class="meta-info">
        <strong>📋 Informations SEO :</strong><br>
        <strong>Title :</strong> {resultat['meta_title']}<br>
        <strong>Description :</strong> {resultat['meta_description']}<br>
        <strong>Mots-clés :</strong> {', '.join(resultat['mots_cles_principaux'] + resultat['mots_cles_secondaires'])}<br>
        <strong>Qualité :</strong> couverture {resultat.get('quality_metrics', {}).get('coverage', 'n/a')}, spécificité {resultat.get('quality_metrics', {}).get('specificity', 'n/a')}, redondance {resultat.get('quality_metrics', {}).get('redundancy', 'n/a')}
    </div>

    {contexte_serp_html}
    {piliers_html}
    {sources_html}

    <h1>{resultat['titre_collection']}</h1>

    {resultat['contenu_html']}
    
    <hr style="margin: 40px 0; border: none; border-top: 2px solid #eee;">
    <p style="color: #999; font-size: 0.9em;">
        Contenu généré le {resultat['date_generation'][:10]} | Mots : ~{resultat['nombre_mots']} | Modèle : {resultat['modele_utilise']}
    </p>
</body>
</html>"""

    with open(chemin_html, 'w', encoding='utf-8') as f:
        f.write(html_complet)

    # Sauvegarder aussi le JSON
    chemin_json = os.path.join(dossier_sortie, f"{nom_fichier_base}_metadata.json")
    with open(chemin_json, 'w', encoding='utf-8') as f:
        json.dump(resultat, f, ensure_ascii=False, indent=2)

    print(f"\n💾 Fichiers sauvegardés :")
    print(f"   - HTML : {chemin_html}")
    print(f"   - JSON : {chemin_json}")

    return chemin_html, chemin_json


# ============================================================================
# EXEMPLES D'UTILISATION & INTERFACE CLI
# ============================================================================

def _executer_generation(
    api_key: str,
    requetes: Sequence[CollectionRequest],
    dossier_sortie: str | os.PathLike[str],
    serp_api_key: Optional[str] = None,
    nb_resultats_serp: int = 50,
) -> None:
    """Génère et sauvegarde une ou plusieurs collections."""

    _preparer_differenciation_collections(requetes)

    for index, requete in enumerate(requetes, start=1):
        print(f"\n--- Collection {index}/{len(requetes)} ---")
        resultat = generer_description_collection(
            api_key=api_key,
            titre_collection=requete.titre_collection,
            mots_cles_principaux=requete.mots_cles_principaux,
            mots_cles_secondaires=requete.mots_cles_secondaires,
            questions_faq=requete.questions_faq,
            infos_supplementaires=requete.infos_supplementaires,
            liens_internes=requete.liens_internes,
            serp_api_key=serp_api_key,
            nb_resultats_serp=nb_resultats_serp,
            requete_serp=requete.requete_serp,
        )
        sauvegarder_html(resultat, dossier_sortie=dossier_sortie)

def _charger_collections_depuis_json(fichier: str | Path) -> List[CollectionRequest]:
    """Charge une ou plusieurs collections depuis un fichier JSON."""

    chemin = Path(fichier)
    if not chemin.exists():
        raise FileNotFoundError(f"Fichier introuvable : {chemin}")

    with chemin.open(encoding='utf-8') as flux:
        donnees = json.load(flux)

    if isinstance(donnees, dict):
        donnees = [donnees]

    requetes: List[CollectionRequest] = []
    for entree in donnees:
        requetes.append(
            CollectionRequest(
                titre_collection=entree.get('titre_collection') or entree.get('titre') or '',
                mots_cles_principaux=entree.get('mots_cles_principaux', []),
                mots_cles_secondaires=entree.get('mots_cles_secondaires', []),
                questions_faq=entree.get('questions_faq', []),
                infos_supplementaires=entree.get('infos_supplementaires'),
                liens_internes=entree.get('liens_internes', []),
                requete_serp=entree.get('requete_serp') or entree.get('serp_query'),
            )
        )
    return requetes

def _parser_arguments() -> argparse.Namespace:
    """Configure l'analyseur d'arguments de ligne de commande."""

    parser = argparse.ArgumentParser(
        description=(
            "Génère des descriptions de pages collection optimisées SEO à partir d'un titre"
        )
    )
    parser.add_argument('--api-key', dest='api_key', help='Clé API OpenAI (sinon OPENAI_API_KEY)')
    parser.add_argument('--titre', help='Titre de la collection à générer')
    parser.add_argument(
        '--titres',
        help='Liste de titres de collections à générer (séparés par des point-virgules)',
    )
    parser.add_argument(
        '--keywords-principaux',
        help='Mots-clés principaux séparés par des virgules',
    )
    parser.add_argument(
        '--keywords-secondaires',
        help='Mots-clés secondaires séparés par des virgules',
    )
    parser.add_argument('--faq', help='Questions FAQ séparées par des point-virgules')
    parser.add_argument('--infos', help='Informations supplémentaires à intégrer')
    parser.add_argument(
        '--liens-internes',
        help='Liens internes suggérés (séparés par des virgules)',
    )
    parser.add_argument(
        '--serp-api-key',
        help='Clé API Google Custom Search personnalisée (sinon GOOGLE_SEARCH_API_KEY)',
    )
    parser.add_argument(
        '--serp-query',
        help='Requête personnalisée pour la SERP (défaut : titre de la collection)',
    )
    parser.add_argument(
        '--serp-results',
        type=int,
        default=50,
        help='Nombre de résultats SERP à analyser (défaut : 50)',
    )
    parser.add_argument(
        '--json',
        help='Fichier JSON contenant les paramètres des collections',
    )
    parser.add_argument(
        '--output-dir',
        default='collections_generees',
        help='Dossier de sortie pour les fichiers générés',
    )
    parser.add_argument(
        '--mode',
        choices=['simple', 'batch', 'interactif'],
        help='Exécuter un des scénarios préconfigurés',
    )
    return parser.parse_args()

def _determiner_api_key(args: argparse.Namespace) -> str:
    """Récupère la clé API depuis les arguments ou les variables d'environnement."""

    api_key = args.api_key or os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise ValueError(
            'Aucune clé API fournie. Utilisez --api-key ou définissez la variable OPENAI_API_KEY.'
        )
    return api_key


def _determiner_serp_api_key(args: argparse.Namespace) -> Optional[str]:
    """Récupère la clé Google Custom Search optionnelle."""

    return (
        args.serp_api_key
        or os.getenv('GOOGLE_SEARCH_API_KEY')
        or os.getenv('SERPAPI_API_KEY')
    )

def exemple_simple():
    """Exemple 1 : Génération simple d'une collection"""

    print('=' * 70)
    print('  EXEMPLE 1 : GÉNÉRATION SIMPLE')
    print('=' * 70)

    # ⚠️ REMPLACE PAR TA CLÉ API OPENAI
    MA_CLE_API = 'sk-proj-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'
    CLE_API_GOOGLE = None  # Optionnel : ajoute ta clé Google Custom Search personnelle

    requete = CollectionRequest(
        titre_collection='Vestes en Cuir Femme',
        mots_cles_principaux=['veste cuir femme', 'veste en cuir'],
        mots_cles_secondaires=['perfecto femme', 'blouson cuir noir', 'veste simili cuir'],
        questions_faq=[
            'Comment choisir la taille de ma veste en cuir ?',
            'Comment entretenir une veste en cuir ?',
            'Le cuir va-t-il se détendre avec le temps ?',
            'Puis-je porter une veste en cuir sous la pluie ?',
        ],
        infos_supplementaires='Nos vestes sont en cuir véritable européen, avec livraison gratuite en France.',
        liens_internes=[
            '/collections/manteaux-femme',
            '/collections/chaussures-cuir-femme',
            '/blog/guide-entretien-cuir',
        ],
    )

    _executer_generation(
        MA_CLE_API,
        [requete],
        'collections_generees',
        serp_api_key=CLE_API_GOOGLE,
        nb_resultats_serp=50,
    )

    print("\n✅ Exemple terminé ! Consulte le dossier 'collections_generees'")

def exemple_batch():
    """Exemple 2 : Génération de plusieurs collections"""

    print('=' * 70)
    print('  EXEMPLE 2 : GÉNÉRATION EN BATCH')
    print('=' * 70)

    # ⚠️ REMPLACE PAR TA CLÉ API OPENAI
    MA_CLE_API = 'sk-proj-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'
    CLE_API_GOOGLE = None  # Optionnel : ajoute ta clé Google Custom Search personnelle

    collections = [
        CollectionRequest(
            titre_collection='Chaussures de Randonnée Femme',
            mots_cles_principaux=['chaussures randonnée femme'],
            mots_cles_secondaires=['trekking', 'imperméable', 'montagne'],
            questions_faq=[
                'Quelle pointure choisir ?',
                'Comment imperméabiliser mes chaussures ?',
                'Quelles chaussettes associer ?',
                'Comment sécher mes chaussures après une sortie ?',
            ],
            liens_internes=[
                '/collections/chaussures-randonnee-homme',
                '/collections/accessoires-trekking',
                '/blog/conseils-trekking',
            ],
        ),
        CollectionRequest(
            titre_collection='Sapins Artificiels',
            mots_cles_principaux=['sapin artificiel', 'sapin de noël'],
            mots_cles_secondaires=['sapin artificiel réaliste', 'faux sapin'],
            infos_supplementaires='Sapins de 120cm à 240cm, aspect naturel garanti.',
            liens_internes=[
                '/collections/guirlandes-lumineuses',
                '/collections/ornements-noel',
                '/blog/guide-decoration-noel',
            ],
        ),
        CollectionRequest(
            titre_collection='Montres Connectées Homme',
            mots_cles_principaux=['montre connectée homme'],
            mots_cles_secondaires=['smartwatch', 'montre sport', 'GPS'],
            questions_faq=[
                'La montre est-elle compatible avec Android et iOS ?',
                'Quelle autonomie espérer au quotidien ?',
                'Les montres sont-elles étanches ?',
                'Quels capteurs de santé sont disponibles ?',
            ],
            liens_internes=[
                '/collections/montres-connectees-femme',
                '/collections/accessoires-connectes',
                '/blog/guide-montre-connectee',
            ],
        ),
    ]

    _executer_generation(
        MA_CLE_API,
        collections,
        'collections_generees',
        serp_api_key=CLE_API_GOOGLE,
        nb_resultats_serp=50,
    )

    print("\n✨ Génération batch terminée !")

def exemple_interactif():
    """Exemple 3 : Mode interactif simple"""

    print('=' * 70)
    print('  GÉNÉRATEUR DE DESCRIPTIONS COLLECTIONS SEO')
    print('=' * 70)

    # Demander la clé API
    api_key = input("\n🔑 Entre ta clé API OpenAI : ").strip()

    if not api_key:
        print('❌ Clé API requise')
        return

    serp_key = input(
        "\n🌐 Entre ta clé API Google Custom Search (laisser vide pour utiliser GOOGLE_SEARCH_API_KEY) : "
    ).strip()
    if not serp_key:
        serp_key = os.getenv('GOOGLE_SEARCH_API_KEY') or os.getenv('SERPAPI_API_KEY') or ''

    # Demander le titre
    titre = input("\n📝 Titre de la collection : ").strip()
    if not titre:
        print('❌ Le titre est obligatoire')
        return

    # Mots-clés principaux
    print("\n💡 Mots-clés principaux (séparés par des virgules) :")
    mots_cles_input = input(f"   (laisse vide pour utiliser '{titre}') : ").strip()
    mots_cles_principaux = [mk.strip() for mk in mots_cles_input.split(',')] if mots_cles_input else None

    # Mots-clés secondaires
    print("\n💡 Mots-clés secondaires (optionnel, séparés par des virgules) :")
    secondaires_input = input('   : ').strip()
    mots_cles_secondaires = [mk.strip() for mk in secondaires_input.split(',')] if secondaires_input else None

    # Infos supplémentaires
    print("\n📋 Informations supplémentaires sur tes produits (optionnel) :")
    infos = input('   : ').strip() or None

    # Liens internes
    print("\n🔗 Liens internes suggérés (optionnel, séparés par des virgules) :")
    liens_input = input('   : ').strip()
    liens = [li.strip() for li in liens_input.split(',')] if liens_input else None

    # Générer
    print("\n⏳ Génération en cours...")
    try:
        requete = CollectionRequest(
            titre_collection=titre,
            mots_cles_principaux=mots_cles_principaux or [],
            mots_cles_secondaires=mots_cles_secondaires or [],
            infos_supplementaires=infos,
            liens_internes=liens or [],
        )

        _executer_generation(
            api_key,
            [requete],
            'collections_generees',
            serp_api_key=serp_key or None,
            nb_resultats_serp=50,
        )

        print("\n✨ Génération réussie !")

    except Exception as e:
        print(f"\n❌ Erreur : {e}")

def _afficher_bienvenue() -> None:
    print("\n" + '=' * 70)
    print('  GÉNÉRATEUR DE DESCRIPTIONS COLLECTIONS E-COMMERCE SEO')
    print('=' * 70)


def _normaliser_depuis_chaine(contenu: Optional[str], separateur: str) -> List[str]:
    if not contenu:
        return []
    return _normaliser_liste(part.strip() for part in contenu.split(separateur))

def main() -> None:
    args = _parser_arguments()
    serp_api_key = _determiner_serp_api_key(args)
    nb_resultats_serp = max(0, args.serp_results)

    try:
        if args.json:
            api_key = _determiner_api_key(args)
            requetes = _charger_collections_depuis_json(args.json)
            _executer_generation(
                api_key,
                requetes,
                args.output_dir,
                serp_api_key=serp_api_key,
                nb_resultats_serp=nb_resultats_serp,
            )
            return

        if args.titres:
            api_key = _determiner_api_key(args)
            titres = _normaliser_depuis_chaine(args.titres, ';')
            if not titres:
                raise ValueError('Aucun titre fourni via --titres')
            requetes = [CollectionRequest(titre_collection=titre) for titre in titres]
            _executer_generation(
                api_key,
                requetes,
                args.output_dir,
                serp_api_key=serp_api_key,
                nb_resultats_serp=nb_resultats_serp,
            )
            return

        if args.titre:
            api_key = _determiner_api_key(args)
            requete = CollectionRequest(
                titre_collection=args.titre,
                mots_cles_principaux=_normaliser_depuis_chaine(
                    args.keywords_principaux, ','
                ),
                mots_cles_secondaires=_normaliser_depuis_chaine(
                    args.keywords_secondaires, ','
                ),
                questions_faq=_normaliser_depuis_chaine(args.faq, ';'),
                infos_supplementaires=args.infos,
                liens_internes=_normaliser_depuis_chaine(args.liens_internes, ','),
                requete_serp=args.serp_query,
            )
            _executer_generation(
                api_key,
                [requete],
                args.output_dir,
                serp_api_key=serp_api_key,
                nb_resultats_serp=nb_resultats_serp,
            )
            return

        if args.mode == 'simple':
            exemple_simple()
        elif args.mode == 'batch':
            exemple_batch()
        elif args.mode == 'interactif':
            exemple_interactif()
        else:
            _afficher_bienvenue()
            print("\nUtilisez --help pour découvrir les options disponibles ou --mode pour lancer un exemple.")
    except Exception as exc:  # noqa: BLE001
        print(f"\n❌ Erreur : {exc}")


if __name__ == '__main__':
    main()
